{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the 'business' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_json(\"yelp_academic_dataset_business.json\", chunksize = 1000, lines = True)\n",
    "drop_cols = ['address', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open','attributes', 'hours']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deleting non useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "chunks = []\n",
    "a = 0\n",
    "for chunk in df2:\n",
    "    a += 1\n",
    "    chunk_b = chunk.drop(drop_cols, axis = 1)\n",
    "    restas = chunk_b[chunk_b['categories'].str.contains('restaurant', case = False, na = False)]\n",
    "    chunks.append(restas)\n",
    "restaurants = pd.concat(chunks, ignore_index= True, join='outer')\n",
    "end = time.time()\n",
    "elapsed = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63961, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the reviews dataset\n",
    "* Remember we made the merge to use ONLY restaurants data, because there were data from other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_raw = pd.read_json(\"yelp_academic_dataset_review.json\", chunksize=100000, lines = True)\n",
    "drop_cols = ['review_id', 'user_id','useful', 'funny', 'cool', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using merge instead of join because we want to join in another column other than the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for chunk in reviews_raw:\n",
    "    a += 1\n",
    "    reviews = chunk.drop(drop_cols, axis = 1)\n",
    "    data = restaurants.merge(reviews, left_on = 'business_id', right_on = 'business_id',how = 'inner')\n",
    "    if a == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally the data to be preprocessed (the \"text\" column, to be exact) \n",
    "TBD:\n",
    "* Delete all number 3 i.e neutral \n",
    "* Same number of positive as negatives\n",
    "* Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>categories</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pQeaRpvuhoEqudo3uymHIQ</td>\n",
       "      <td>The Empanadas House</td>\n",
       "      <td>Champaign</td>\n",
       "      <td>Ethnic Food, Food Trucks, Specialty Food, Impo...</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the empanadas from the Empanadas House!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CsLQLiRoafpJPJSkNX2h5Q</td>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Food, Restaurants, Grocery, Middle Eastern</td>\n",
       "      <td>3</td>\n",
       "      <td>Definitely under new management, and the dinin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CsLQLiRoafpJPJSkNX2h5Q</td>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Food, Restaurants, Grocery, Middle Eastern</td>\n",
       "      <td>3</td>\n",
       "      <td>I will also agree that this place has great fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lu7vtrp_bE9PnxWfA8g4Pg</td>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>Thornhill</td>\n",
       "      <td>Japanese, Fast Food, Food Court, Restaurants</td>\n",
       "      <td>4</td>\n",
       "      <td>Been coming here since I was in grade 9 so abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vjTVxnsQEZ34XjYNS-XUpA</td>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Food, Pretzels, Bakeries, Fast Food, Restaurants</td>\n",
       "      <td>5</td>\n",
       "      <td>Love Wetzel's pretzels! I always get them when...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 name       city  \\\n",
       "0  pQeaRpvuhoEqudo3uymHIQ  The Empanadas House  Champaign   \n",
       "1  CsLQLiRoafpJPJSkNX2h5Q     Middle East Deli  Charlotte   \n",
       "2  CsLQLiRoafpJPJSkNX2h5Q     Middle East Deli  Charlotte   \n",
       "3  lu7vtrp_bE9PnxWfA8g4Pg         Banzai Sushi  Thornhill   \n",
       "4  vjTVxnsQEZ34XjYNS-XUpA    Wetzel's Pretzels    Phoenix   \n",
       "\n",
       "                                          categories  stars  \\\n",
       "0  Ethnic Food, Food Trucks, Specialty Food, Impo...      5   \n",
       "1         Food, Restaurants, Grocery, Middle Eastern      3   \n",
       "2         Food, Restaurants, Grocery, Middle Eastern      3   \n",
       "3       Japanese, Fast Food, Food Court, Restaurants      4   \n",
       "4   Food, Pretzels, Bakeries, Fast Food, Restaurants      5   \n",
       "\n",
       "                                                text  \n",
       "0  I love the empanadas from the Empanadas House!...  \n",
       "1  Definitely under new management, and the dinin...  \n",
       "2  I will also agree that this place has great fo...  \n",
       "3  Been coming here since I was in grade 9 so abo...  \n",
       "4  Love Wetzel's pretzels! I always get them when...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a toyset to work in trials from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Empanadas House</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the empanadas from the Empanadas House!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>3</td>\n",
       "      <td>Definitely under new management, and the dinin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>3</td>\n",
       "      <td>I will also agree that this place has great fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>4</td>\n",
       "      <td>Been coming here since I was in grade 9 so abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>5</td>\n",
       "      <td>Love Wetzel's pretzels! I always get them when...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  stars  \\\n",
       "0  The Empanadas House      5   \n",
       "1     Middle East Deli      3   \n",
       "2     Middle East Deli      3   \n",
       "3         Banzai Sushi      4   \n",
       "4    Wetzel's Pretzels      5   \n",
       "\n",
       "                                                text  \n",
       "0  I love the empanadas from the Empanadas House!...  \n",
       "1  Definitely under new management, and the dinin...  \n",
       "2  I will also agree that this place has great fo...  \n",
       "3  Been coming here since I was in grade 9 so abo...  \n",
       "4  Love Wetzel's pretzels! I always get them when...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data.loc[0:99, ['name', 'stars', 'text']]\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some graphs to know the number of reviews by ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1e93af50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bars = data2['stars'].value_counts()\n",
    "sns.barplot(x = bars.index, y = bars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Delete the number 3's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data2.drop(data2[data2['stars'] == 3].index, inplace = True)\n",
    "data2 = data2.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We want just the reviews with 4-5 to be positive and the 1-2 to be negative, we do that on the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['Sentiment'] = data2['stars'].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name         object\n",
       "stars         int64\n",
       "text         object\n",
       "Sentiment     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    56\n",
       "0    44\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     I love the empanadas from the Empanadas House!...\n",
      "1     Definitely under new management, and the dinin...\n",
      "2     I will also agree that this place has great fo...\n",
      "3     Been coming here since I was in grade 9 so abo...\n",
      "4     Love Wetzel's pretzels! I always get them when...\n",
      "5     So why would I be giving a Fast Food chain loc...\n",
      "6     I come here bout 3x's a mo. & I just can't do ...\n",
      "7     We've eaten here a few times now, and have bee...\n",
      "8     This place used to be called Chicken Bonz, as ...\n",
      "9     Good food, terrible customer service.  For me ...\n",
      "10    Just another nice Amir food nothing more but r...\n",
      "11    Small portions and expensive. There was so lit...\n",
      "12    I only ever eat at three of the millions of Am...\n",
      "13    We just recently discovered this place and I j...\n",
      "14    I can't believe I never knew about a place tha...\n",
      "15    The one thing keeping this place from getting ...\n",
      "16    We've tried a few different Chinese delivery p...\n",
      "17    My expectations of chinese delivery places in ...\n",
      "18    This place only gets one star because the syst...\n",
      "19    This place is exactly why I rarely eat Chinese...\n",
      "20    Tzikii's food is awesome. I absolutely love th...\n",
      "21    I am rating this Five Guys as a comparison to ...\n",
      "22    By far the best burger joint in Charlotte.  I'...\n",
      "23    These Guys just get it right,great burgers str...\n",
      "24    Always good fresh juicy burgers. They are a bi...\n",
      "25    I am a fan of Five Guys where ever I'm at... a...\n",
      "26    Stopped here with my wife due to the 18\" pizza...\n",
      "27    Delicious delicous declicous!! It is my go-to ...\n",
      "28    As a Vietnamese person, I highly recommend thi...\n",
      "29    Been going to Mi Mi Restaurant for years! I lo...\n",
      "30    Our go-to place for Vietnamese food! This plac...\n",
      "31    I honestly think that anyone would enjoy food ...\n",
      "32    Good Day to all reading my review of Mimi's Th...\n",
      "33    the bun and fish sauce were awesome.\\nthe pho ...\n",
      "34    Nice service. Ordered vermicelli grilled pork,...\n",
      "35    Best BBQ chicken and pork on rice, and nice se...\n",
      "36    ordered a rice dish. the bbq pork chop and bon...\n",
      "37    Im torn about this place.  I've been a few tim...\n",
      "38    Typical Asian fare, good price, fair service b...\n",
      "39    Service is the best part of this place. Nice, ...\n",
      "40    Bomb food yo!!! So good that I didn't even com...\n",
      "41    The best Pho I've had in the city.  My family ...\n",
      "42    Everyone tells me how much they love Mi Mi.  I...\n",
      "43    I rate 2.5 stars and have deleted my previous ...\n",
      "44    I ordered the combo beef, shrimp, and spring r...\n",
      "45    If you happen to be in the area and feel like ...\n",
      "46    Service and prices are 5 stars.\\n\\nVermicelli ...\n",
      "47    The owners here are all very friendly. I go to...\n",
      "48    Ever since the Pho 88 at Broadview and Gerrard...\n",
      "49    Get the rice paper roll platters. For $13, it'...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data2['text'][0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here start the NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to make a bag of words, it can be done manually, but also with sklearn.\n",
    "\n",
    "**Steps:** I am trying to extract the nest tokens with the tokenizer from Potts and then feed that already \"clean tokens\" to the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = data2['text'][1:3]\n",
    "raw\n",
    "raw2 = data2['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "counter = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n",
      "e\n",
      "f\n",
      "i\n",
      "n\n",
      "i\n",
      "t\n",
      "e\n",
      "l\n",
      "y\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      " \n",
      "m\n",
      "a\n",
      "n\n",
      "a\n",
      "g\n",
      "e\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "d\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      "a\n",
      " \n",
      "h\n",
      "a\n",
      "s\n",
      " \n",
      "b\n",
      "e\n",
      "e\n",
      "n\n",
      " \n",
      "t\n",
      "o\n",
      "t\n",
      "a\n",
      "l\n",
      "l\n",
      "y\n",
      " \n",
      "r\n",
      "e\n",
      "d\n",
      "o\n",
      "n\n",
      "e\n",
      ".\n",
      " \n",
      "B\n",
      "i\n",
      "g\n",
      ",\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "f\n",
      "y\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "i\n",
      "r\n",
      "s\n",
      ",\n",
      " \n",
      "d\n",
      "a\n",
      "n\n",
      "g\n",
      "l\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "b\n",
      "i\n",
      "s\n",
      "t\n",
      "r\n",
      "o\n",
      " \n",
      "l\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      ",\n",
      " \n",
      "i\n",
      "t\n",
      "'\n",
      "s\n",
      " \n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      "w\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "f\n",
      "i\n",
      "e\n",
      "r\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "e\n",
      "n\n",
      "u\n",
      " \n",
      "h\n",
      "a\n",
      "s\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "n\n",
      "g\n",
      "e\n",
      "d\n",
      " \n",
      "q\n",
      "u\n",
      "i\n",
      "t\n",
      "e\n",
      " \n",
      "a\n",
      " \n",
      "b\n",
      "i\n",
      "t\n",
      " \n",
      "-\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "r\n",
      "n\n",
      "s\n",
      " \n",
      "m\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      "w\n",
      "a\n",
      "r\n",
      "d\n",
      "s\n",
      " \n",
      "T\n",
      "u\n",
      "r\n",
      "k\n",
      "i\n",
      "s\n",
      "h\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "a\n",
      "n\n",
      "y\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "e\n",
      "l\n",
      "s\n",
      "e\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "h\n",
      "a\n",
      "s\n",
      " \n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "e\n",
      "s\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "o\n",
      "p\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "m\n",
      "e\n",
      "a\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      ".\n",
      " \n",
      "V\n",
      "e\n",
      "g\n",
      " \n",
      "f\n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "h\n",
      "a\n",
      "s\n",
      "n\n",
      "'\n",
      "t\n",
      " \n",
      "b\n",
      "u\n",
      "d\n",
      "g\n",
      "e\n",
      "d\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "n\n",
      "e\n",
      "i\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "r\n",
      "i\n",
      "c\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "b\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "q\n",
      "u\n",
      "a\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "s\n",
      "i\n",
      "z\n",
      "e\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "e\n",
      "d\n",
      " \n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      ".\n",
      " \n",
      "F\n",
      "a\n",
      "l\n",
      "a\n",
      "f\n",
      "e\n",
      "l\n",
      " \n",
      "n\n",
      "o\n",
      "w\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "e\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "m\n",
      "i\n",
      "d\n",
      "g\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "m\n",
      "u\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "i\n",
      "d\n",
      "e\n",
      ",\n",
      " \n",
      "w\n",
      "h\n",
      "i\n",
      "c\n",
      "h\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      " \n",
      "n\n",
      "i\n",
      "c\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      "u\n",
      "c\n",
      "h\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "E\n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "w\n",
      "a\n",
      "i\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "m\n",
      "e\n",
      " \n",
      "w\n",
      "a\n",
      "s\n",
      " \n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      " \n",
      "t\n",
      "a\n",
      "l\n",
      "l\n",
      ",\n",
      " \n",
      "f\n",
      "e\n",
      "m\n",
      "a\n",
      "l\n",
      "e\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "b\n",
      "e\n",
      "a\n",
      "u\n",
      "t\n",
      "i\n",
      "f\n",
      "u\n",
      "l\n",
      ",\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "a\n",
      "w\n",
      "e\n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "M\n",
      "e\n",
      "d\n",
      "i\n",
      "t\n",
      "e\n",
      "r\n",
      "r\n",
      "a\n",
      "n\n",
      "e\n",
      "a\n",
      "n\n",
      " \n",
      "w\n",
      "a\n",
      "y\n",
      ",\n",
      " \n",
      "s\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      "'\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "tokenizer = []\n",
    "a = 0\n",
    "for i in raw2:\n",
    "    words = i.lower()\n",
    "    words = words.split()\n",
    "    for word in words:\n",
    "        a += 1\n",
    "        counter[word] += 1\n",
    "        if word not in tokenizer:\n",
    "            tokenizer.append(word)\n",
    "len(tokenizer)\n",
    "tokenizer\n",
    "words = raw2.split()\n",
    "for word in words:\n",
    "    a += 1\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the negation tagging, put the negation until ^[.:;!?]$ (until the punctuation mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#out of the box tokenizer and counter\n",
    "from nltk.probability import FreqDist\n",
    "counter = FreqDist()\n",
    "tokens_nltk = word_tokenize(raw2)\n",
    "\n",
    "tokenizer = []\n",
    "for word in tokens_nltk:\n",
    "    counter[word.lower()] += 1\n",
    "\n",
    "len(tokens_nltk)\n",
    "len(counter)\n",
    "counter['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 12),\n",
       " ('.', 6),\n",
       " ('and', 5),\n",
       " ('the', 5),\n",
       " ('has', 4),\n",
       " ('a', 3),\n",
       " ('that', 3),\n",
       " (\"'s\", 2),\n",
       " ('some', 2),\n",
       " ('have', 2),\n",
       " ('on', 2),\n",
       " ('definitely', 1),\n",
       " ('under', 1),\n",
       " ('new', 1),\n",
       " ('management', 1),\n",
       " ('dining', 1),\n",
       " ('area', 1),\n",
       " ('been', 1),\n",
       " ('totally', 1),\n",
       " ('redone', 1)]"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.B() #is the number of unique words (?)\n",
    "x = counter.N #is the number of words\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD:\n",
    "### Check the paper that is mentioned in notion by UCLondon\n",
    "\n",
    "* Delete all the reviews that are not in english\n",
    "* Same number of negative that as positive reviews for the sets\n",
    "* Tokenization and BoW creation (**BoW with frequency or with presence?**)\n",
    "* unigrams and bigrams\n",
    "* lower the case\n",
    "* POS tagging?\n",
    "* EDA como en potts con las palabras mas frecuentes en positives and negatives reviews\n",
    "* Handling negation? _NOT or with sentiment negative scoring?\n",
    "* See the book by Bing Lui for how to identify fake news, resonates with anomaly detection and identifying if a review is fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123456789', '987654321', '987654321']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Definitely under new management, and the dining area has been totally redone. Big, comfy chairs, dangling bistro lighting, it's somewhat comfier.\\n\\nThe menu has changed quite a bit - this learns more towards Turkish than anything else, and has some interesting options for you meaters. Veg fare hasn't budged, and neither have the prices, but the quality and portion size have improved some. Falafel now comes with a smidge of hummus on the side, which is a nice touch.\\n\\nEveryone that waited on me was very tall, female, and beautiful, in that awesome Mediterranean way, so there's that.\""
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\"Hello my Number is 123456789 and  \n",
    "             my friend's number is 987654321, and my number is also 987654321\"\"\"\n",
    "    \n",
    "# A sample regular expression to find digits.  \n",
    "regex = '\\d+'             \n",
    "    \n",
    "match = re.findall(regex, string)  \n",
    "print(match)\n",
    "raw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = re.search('under' , raw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no']"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg = re.compile(negation_string, re.VERBOSE | re.I | re.UNICODE)\n",
    "string = \"no\"\n",
    "word = neg.findall(string)\n",
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation tagging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesimist(text):\n",
    "    x = text\n",
    "    x = x.split()\n",
    "    hasta = 0\n",
    "    desde = 0\n",
    "    c = 0\n",
    "    j = 0\n",
    "    passer = False\n",
    "    for k in range(len(x)):\n",
    "        #print(j)\n",
    "        #print(k)\n",
    "        #if not passer:\n",
    "            #continue\n",
    "        passer = True\n",
    "\n",
    "        i = x[k]\n",
    "        negation_string = r\"\"\"\n",
    "        ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "            havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "            wont|wouldnt|but|doesnt|didnt|isnt|arent|aint\n",
    "        )$|n't\n",
    "        \"\"\"\n",
    "        #print(\"first\")\n",
    "        c +=1\n",
    "        #match = re.search(r'\\bthe\\b',i)\n",
    "        neg = re.compile(negation_string, re.VERBOSE | re.I | re.UNICODE)\n",
    "        match = neg.findall(i)\n",
    "        #print(i + \"i\")\n",
    "        #print('desde', desde)\n",
    "        #print(f'hasta menos desde es: {hasta - desde} y c es {c}')\n",
    "        if c < (hasta - desde):\n",
    "            continue\n",
    "        if match:\n",
    "            c = 0\n",
    "            desde = k + 1\n",
    "            #print(c - 1)\n",
    "            #j = i\n",
    "            jump = k\n",
    "            for j in range(jump, 100):\n",
    "\n",
    "                #print(j)\n",
    "                try:\n",
    "                    comma = re.search(r'[.:;!?]', x[j])\n",
    "                    #print(\"second\")\n",
    "                    if comma:\n",
    "                        hasta = j + 1\n",
    "                        c += 1\n",
    "                        \n",
    "                        #this try to avoid error if there is no punctuation error before the phrase ends\n",
    "                        try:\n",
    "                            for i in range(desde, hasta):\n",
    "                                repl = re.match(r'\\w+', x[i])\n",
    "                                x[i] = repl.group() + \"_NOT\"\n",
    "                            \n",
    "                            c = 0\n",
    "                            break\n",
    "                        except:\n",
    "                            #print(' '.join(x))\n",
    "                            c = 0\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "        if match:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "                #continue\n",
    "\n",
    "\n",
    "            #else:\n",
    "                #continue\n",
    "            #break\n",
    "    xx = ' '.join(x)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally the negation tagging is working, next steps:\n",
    "    1. Identify all the negation words on the regular expresion, can be taken from one paper.\n",
    "    2. Implement it i conjuction with the tokenizer and the stop words removal\n",
    "    3. run it for all the dataset\n",
    "    4. Balance the positive and negative classes on the data set that we are going to take to make all the trials.\n",
    "    5. Finish the identification of features\n",
    "    6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current status\n",
    "\n",
    "order of the pipeline (in **bold** what is already done)\n",
    "\n",
    "1. **Negator**\n",
    "2. **Tokenizer**\n",
    "3. Stop words removal (kind of done, have to figure it out)\n",
    "3. BoW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name         0\n",
       "stars        0\n",
       "text         0\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial = data2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial[\"sample\"] = trial.loc[:, \"text\"].apply(pesimist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Empanadas House</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the empanadas from the Empanadas House!...</td>\n",
       "      <td>1</td>\n",
       "      <td>I love the empanadas from the Empanadas House!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>3</td>\n",
       "      <td>Definitely under new management, and the dinin...</td>\n",
       "      <td>0</td>\n",
       "      <td>Definitely under new management, and the dinin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>3</td>\n",
       "      <td>I will also agree that this place has great fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>I will also agree that this place has great fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>4</td>\n",
       "      <td>Been coming here since I was in grade 9 so abo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Been coming here since I was in grade 9 so abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>5</td>\n",
       "      <td>Love Wetzel's pretzels! I always get them when...</td>\n",
       "      <td>1</td>\n",
       "      <td>Love Wetzel's pretzels! I always get them when...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  stars  \\\n",
       "0  The Empanadas House      5   \n",
       "1     Middle East Deli      3   \n",
       "2     Middle East Deli      3   \n",
       "3         Banzai Sushi      4   \n",
       "4    Wetzel's Pretzels      5   \n",
       "\n",
       "                                                text  Sentiment  \\\n",
       "0  I love the empanadas from the Empanadas House!...          1   \n",
       "1  Definitely under new management, and the dinin...          0   \n",
       "2  I will also agree that this place has great fo...          0   \n",
       "3  Been coming here since I was in grade 9 so abo...          1   \n",
       "4  Love Wetzel's pretzels! I always get them when...          1   \n",
       "\n",
       "                                              sample  \n",
       "0  I love the empanadas from the Empanadas House!...  \n",
       "1  Definitely under new management, and the dinin...  \n",
       "2  I will also agree that this place has great fo...  \n",
       "3  Been coming here since I was in grade 9 so abo...  \n",
       "4  Love Wetzel's pretzels! I always get them when...  "
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.happyfuntokenizing import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tok.tokenize(raw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<map at 0x1a2979bd10>,\n",
       " ['definitely',\n",
       "  'under',\n",
       "  'new',\n",
       "  'management',\n",
       "  ',',\n",
       "  'and',\n",
       "  'the',\n",
       "  'dining',\n",
       "  'area',\n",
       "  'has',\n",
       "  'been',\n",
       "  'totally',\n",
       "  'redone',\n",
       "  '.',\n",
       "  'big',\n",
       "  ',',\n",
       "  'comfy',\n",
       "  'chairs',\n",
       "  ',',\n",
       "  'dangling',\n",
       "  'bistro',\n",
       "  'lighting',\n",
       "  ',',\n",
       "  \"it's\",\n",
       "  'somewhat',\n",
       "  'comfier',\n",
       "  '.',\n",
       "  'the',\n",
       "  'menu',\n",
       "  'has',\n",
       "  'changed',\n",
       "  'quite',\n",
       "  'a',\n",
       "  'bit',\n",
       "  '-',\n",
       "  'this',\n",
       "  'learns',\n",
       "  'more',\n",
       "  'towards',\n",
       "  'turkish',\n",
       "  'than',\n",
       "  'anything',\n",
       "  'else',\n",
       "  ',',\n",
       "  'and',\n",
       "  'has',\n",
       "  'some',\n",
       "  'interesting',\n",
       "  'options',\n",
       "  'for',\n",
       "  'you',\n",
       "  'meaters',\n",
       "  '.',\n",
       "  'veg',\n",
       "  'fare',\n",
       "  \"hasn't\",\n",
       "  'budged',\n",
       "  ',',\n",
       "  'and',\n",
       "  'neither',\n",
       "  'have',\n",
       "  'the',\n",
       "  'prices',\n",
       "  ',',\n",
       "  'but',\n",
       "  'the',\n",
       "  'quality',\n",
       "  'and',\n",
       "  'portion',\n",
       "  'size',\n",
       "  'have',\n",
       "  'improved',\n",
       "  'some',\n",
       "  '.',\n",
       "  'falafel',\n",
       "  'now',\n",
       "  'comes',\n",
       "  'with',\n",
       "  'a',\n",
       "  'smidge',\n",
       "  'of',\n",
       "  'hummus',\n",
       "  'on',\n",
       "  'the',\n",
       "  'side',\n",
       "  ',',\n",
       "  'which',\n",
       "  'is',\n",
       "  'a',\n",
       "  'nice',\n",
       "  'touch',\n",
       "  '.',\n",
       "  'everyone',\n",
       "  'that',\n",
       "  'waited',\n",
       "  'on',\n",
       "  'me',\n",
       "  'was',\n",
       "  'very',\n",
       "  'tall',\n",
       "  ',',\n",
       "  'female',\n",
       "  ',',\n",
       "  'and',\n",
       "  'beautiful',\n",
       "  ',',\n",
       "  'in',\n",
       "  'that',\n",
       "  'awesome',\n",
       "  'mediterranean',\n",
       "  'way',\n",
       "  ',',\n",
       "  'so',\n",
       "  \"there's\",\n",
       "  'that',\n",
       "  '.'],\n",
       " [\"hasn't\", 'but'])"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.happyfuntokenizing import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial[\"tokens\"] = trial.loc[:, \"sample\"].apply(tok.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<map object at 0x1a22f6e0d0>, ['this', 'place', 'used', 'to', 'be', 'called', 'chicken', 'bonz', ',', 'as', 'they', 'specialized', 'in', 'chicken', 'wings', 'and', 'had', 'a', 'few', 'other', 'items', 'on', 'the', 'side', '.', 'they', 'were', 'pretty', 'good', 'and', 'i', 'ate', 'there', 'all', 'the', 'time', '.', 'used', 'to', 'being', 'the', 'important', 'part', '.', 'a', 'few', 'months', 'ago', 'they', 'broke', 'from', 'the', 'franchise', 'and', 'renamed', 'meat', 'chix', 'and', 'wieners', '.', 'really', '?', 'who', 'calls', 'a', 'restaurant', 'this', '?', 'i', 'tried', 'it', 'out', 'a', 'few', 'times', 'after', 'the', 'change', 'but', 'today_not', 'was_not', 'the_not', 'last_not', 'straw_not', 'we', 'walked', 'in', 'and', 'ordered', 'the', 'usual', ',', '6', 'wings', 'medium', 'for', 'me', ',', 'same', 'for', 'my', 'wife', ',', 'ranch', 'and', 'blue', 'cheese', 'dip', ',', 'large', 'fries', 'to', 'share', 'and', 'two', 'drinks', '.', 'well', 'they', 'changed', 'their', 'menu', 'and', 'no', 'longer', 'have', 'medium', 'sauce', 'or', 'blue', 'cheese', '.', 'what', 'kind', 'of', 'wing', 'place', \"doesn't\", 'have', 'blue', 'cheese', '.', 'my', 'wife', 'changes', 'her', 'order', 'to', 'a', 'portabello', 'burger', 'with', 'onion', 'rings', ',', 'hot', 'sauce', 'on', 'the', 'side', '.', 'almost', '$', '25', 'dollars', 'later', 'we', 'sit', 'down', 'to', 'wait', '.', 'and', 'wait', '...', 'and', 'wait', 'some', 'more', '.', '25', 'minutes', 'later', 'we', 'get', 'our', 'order', '.', 'did', 'i', 'mention', 'this', 'is', 'a', 'cook', 'to', 'order', 'fast-food', 'place', '.', 'orders', 'used', 'to', 'come', 'out', 'at', 'lightning', 'speed', ',', 'sometimes', 'before', 'we', 'had', 'even', 'filled', 'our', 'drinks', 'and', 'sat', 'down', '.', 'not', 'anymore', '.', 'we', 'were', 'assured', 'several', 'times', 'that', 'our', 'food', 'would', 'be', 'right', 'out', 'by', 'who', 'we', 'think', 'was', 'the', 'manager', '.', 'however', 'she', 'grabbed', 'her', 'bags', 'from', 'the', 'office', 'that', 'was', 'set', 'up', 'in', 'a', 'booth', 'in', 'the', 'corner', 'of', 'the', 'restaurant', 'before', 'we', 'got', 'our', 'food', '.', 'after', 'all', 'that', 'waiting', 'you', 'think', 'they', 'could', 'at', 'least', 'get', 'the', 'order', 'correct', ',', 'right', '?', 'nope', '.', 'my', 'wife', 'got', 'her', 'burger', 'with', 'their', 'trendy', 'version', 'of', 'french', 'fries', 'that', 'look', 'like', 'little', 'more', 'than', 'thick', 'potato', 'chips', '.', 'no', 'hot', 'sauce', ',', 'no', 'onion', 'rings', '.', 'i', 'get', 'my', '5', '1/2', 'wings', '(', 'one', 'was', 'a', 'mutant', 'or', 'from', 'a', 'baby', 'chicken', ')', ',', 'ranch', 'and', 'no', 'fries', '.', 'she', 'goes', 'to', 'cashier', '(', 'the', 'only', 'person', 'no', 'working', 'outside', 'of', 'the', 'kitchen', 'btw', ')', 'and', 'let', 'her', 'know', '.', 'a', 'few', 'minutes', 'later', 'we', 'get', 'the', 'onion', 'rings', ',', 'but', 'no', 'sauce', 'or', 'apology', '.', 'the', 'food', 'itself', 'was', 'ok', ',', 'but', 'again', 'used', 'to', 'be', 'much', 'better', '.', 'the', 'wings', 'were', 'small', 'and', 'thinly', 'sauced', '.', 'the', 'order', 'of', '\"', 'fries', '\"', 'was', 'small', 'and', 'not', 'particularly', 'good', '.', 'my', 'wife', 'did', 'say', 'the', 'portabello', 'burger', 'was', 'good', '.', 'i', 'really', 'wish', 'i', \"didn't\", 'have', 'to', 'write', 'this', ',', 'since', 'the', 'previous', 'establishment', 'had', 'been', 'so', 'good', 'and', 'we', 'went', 'there', 'often', '.', 'whatever', 'changed', 'in', 'the', 'past', 'few', 'months', 'has', 'certainly', 'been', 'a', 'bad', 'move', 'on', 'meat', 'chix', 'and', \"wiener's\", 'part', ',', 'and', 'i', \"don't\", 'think', \"they'll\", 'be', 'seeing', 'us', 'again', '.'], ['but', 'no', \"doesn't\", 'not', 'no', 'no', 'no', 'no', 'but', 'no', 'but', 'not', \"didn't\", \"don't\"])\n"
     ]
    }
   ],
   "source": [
    "print(trial.tokens[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, o, k = tok.tokenize(raw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['definitely',\n",
       " 'under',\n",
       " 'new',\n",
       " 'management',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'dining',\n",
       " 'area',\n",
       " 'has',\n",
       " 'been',\n",
       " 'totally',\n",
       " 'redone',\n",
       " '.',\n",
       " 'big',\n",
       " ',',\n",
       " 'comfy',\n",
       " 'chairs',\n",
       " ',',\n",
       " 'dangling',\n",
       " 'bistro',\n",
       " 'lighting',\n",
       " ',',\n",
       " \"it's\",\n",
       " 'somewhat',\n",
       " 'comfier',\n",
       " '.',\n",
       " 'the',\n",
       " 'menu',\n",
       " 'has',\n",
       " 'changed',\n",
       " 'quite',\n",
       " 'a',\n",
       " 'bit',\n",
       " '-',\n",
       " 'this',\n",
       " 'learns',\n",
       " 'more',\n",
       " 'towards',\n",
       " 'turkish',\n",
       " 'than',\n",
       " 'anything',\n",
       " 'else',\n",
       " ',',\n",
       " 'and',\n",
       " 'has',\n",
       " 'some',\n",
       " 'interesting',\n",
       " 'options',\n",
       " 'for',\n",
       " 'you',\n",
       " 'meaters',\n",
       " '.',\n",
       " 'veg',\n",
       " 'fare',\n",
       " \"hasn't\",\n",
       " 'budged',\n",
       " ',',\n",
       " 'and',\n",
       " 'neither',\n",
       " 'have',\n",
       " 'the',\n",
       " 'prices',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'and',\n",
       " 'portion',\n",
       " 'size',\n",
       " 'have',\n",
       " 'improved',\n",
       " 'some',\n",
       " '.',\n",
       " 'falafel',\n",
       " 'now',\n",
       " 'comes',\n",
       " 'with',\n",
       " 'a',\n",
       " 'smidge',\n",
       " 'of',\n",
       " 'hummus',\n",
       " 'on',\n",
       " 'the',\n",
       " 'side',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'touch',\n",
       " '.',\n",
       " 'everyone',\n",
       " 'that',\n",
       " 'waited',\n",
       " 'on',\n",
       " 'me',\n",
       " 'was',\n",
       " 'very',\n",
       " 'tall',\n",
       " ',',\n",
       " 'female',\n",
       " ',',\n",
       " 'and',\n",
       " 'beautiful',\n",
       " ',',\n",
       " 'in',\n",
       " 'that',\n",
       " 'awesome',\n",
       " 'mediterranean',\n",
       " 'way',\n",
       " ',',\n",
       " 'so',\n",
       " \"there's\",\n",
       " 'that',\n",
       " '.']"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
