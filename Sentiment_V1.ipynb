{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the 'business' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_json(\"yelp_academic_dataset_business.json\", chunksize = 1000, lines = True)\n",
    "drop_cols = ['address', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open','attributes', 'hours']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deleting non useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "chunks = []\n",
    "a = 0\n",
    "for chunk in df2:\n",
    "    a += 1\n",
    "    chunk_b = chunk.drop(drop_cols, axis = 1)\n",
    "    restas = chunk_b[chunk_b['categories'].str.contains('restaurant', case = False, na = False)]\n",
    "    chunks.append(restas)\n",
    "restaurants = pd.concat(chunks, ignore_index= True, join='outer')\n",
    "end = time.time()\n",
    "elapsed = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63961, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the reviews dataset\n",
    "* Remember we made the merge to use ONLY restaurants data, because there were data from other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_raw = pd.read_json(\"yelp_academic_dataset_review.json\", chunksize=100000, lines = True)\n",
    "drop_cols = ['review_id', 'user_id','useful', 'funny', 'cool', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using merge instead of join because we want to join in another column other than the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for chunk in reviews_raw:\n",
    "    a += 1\n",
    "    reviews = chunk.drop(drop_cols, axis = 1)\n",
    "    data = restaurants.merge(reviews, left_on = 'business_id', right_on = 'business_id',how = 'inner')\n",
    "    if a == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally the data to be preprocessed (the \"text\" column, to be exact) \n",
    "TBD:\n",
    "* Delete all number 3 i.e neutral \n",
    "* Same number of positive as negatives\n",
    "* Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>categories</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pQeaRpvuhoEqudo3uymHIQ</td>\n",
       "      <td>The Empanadas House</td>\n",
       "      <td>Champaign</td>\n",
       "      <td>Ethnic Food, Food Trucks, Specialty Food, Impo...</td>\n",
       "      <td>5</td>\n",
       "      <td>I ordered feta cheese and spinach empanadas an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CsLQLiRoafpJPJSkNX2h5Q</td>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Food, Restaurants, Grocery, Middle Eastern</td>\n",
       "      <td>2</td>\n",
       "      <td>I've eaten at this location since 1997/98, so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vjTVxnsQEZ34XjYNS-XUpA</td>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Food, Pretzels, Bakeries, Fast Food, Restaurants</td>\n",
       "      <td>5</td>\n",
       "      <td>Wetzels's Pretzels is definitely amazing and v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fnZrZlqW1Z8iWgTVDfv_MA</td>\n",
       "      <td>Carl's Jr</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Mexican, Restaurants, Fast Food</td>\n",
       "      <td>1</td>\n",
       "      <td>WORST experience EVER!!!!! never have i ate an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fnZrZlqW1Z8iWgTVDfv_MA</td>\n",
       "      <td>Carl's Jr</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Mexican, Restaurants, Fast Food</td>\n",
       "      <td>4</td>\n",
       "      <td>Hot fresh food usually.  Staff seems to turn o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 name       city  \\\n",
       "0  pQeaRpvuhoEqudo3uymHIQ  The Empanadas House  Champaign   \n",
       "1  CsLQLiRoafpJPJSkNX2h5Q     Middle East Deli  Charlotte   \n",
       "2  vjTVxnsQEZ34XjYNS-XUpA    Wetzel's Pretzels    Phoenix   \n",
       "3  fnZrZlqW1Z8iWgTVDfv_MA            Carl's Jr  Las Vegas   \n",
       "4  fnZrZlqW1Z8iWgTVDfv_MA            Carl's Jr  Las Vegas   \n",
       "\n",
       "                                          categories  stars  \\\n",
       "0  Ethnic Food, Food Trucks, Specialty Food, Impo...      5   \n",
       "1         Food, Restaurants, Grocery, Middle Eastern      2   \n",
       "2   Food, Pretzels, Bakeries, Fast Food, Restaurants      5   \n",
       "3                    Mexican, Restaurants, Fast Food      1   \n",
       "4                    Mexican, Restaurants, Fast Food      4   \n",
       "\n",
       "                                                text  \n",
       "0  I ordered feta cheese and spinach empanadas an...  \n",
       "1  I've eaten at this location since 1997/98, so ...  \n",
       "2  Wetzels's Pretzels is definitely amazing and v...  \n",
       "3  WORST experience EVER!!!!! never have i ate an...  \n",
       "4  Hot fresh food usually.  Staff seems to turn o...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a toyset to work in trials from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66729, 3)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.loc[:, ['name', 'stars', 'text']]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some graphs to know the number of reviews by ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Delete the number 3's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.drop(data[data['stars'] == 3].index, inplace = True)\n",
    "data = data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We want just the reviews with 4-5 to be positive and the 1-2 to be negative, we do that on the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    44431\n",
       "0    13695\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'] = data['stars'].apply(lambda x: 1 if x > 3 else 0)\n",
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use \"3\" as negative to augment the data and  balance the classes? (optional)\n",
    "One could argue that the reviews that have not unconditionally positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data2.copy()\n",
    "neg_half = list((data3[data3['stars'] <=3]).index)\n",
    "pos_half = list(set(range(data3.shape[0])) - set(neg_half))\n",
    "pos_half = pos_half[0:len(neg_half)]\n",
    "pos_half.extend(neg_half)\n",
    "dropper = list(set(range(data3.shape[0])) - set(pos_half))\n",
    "data3.drop(index = dropper, inplace = True)\n",
    "data3['Sentiment'] = data3['stars'].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3['Sentiment'].value_counts()\n",
    "data2 = data3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here start the NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to make a bag of words, it can be done manually, but also with sklearn.\n",
    "\n",
    "**Steps:** I am trying to extract the nest tokens with the tokenizer from Potts and then feed that already \"clean tokens\" to the vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC for NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = data2['text'][1:3]\n",
    "raw\n",
    "raw2 = data2['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "counter = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = []\n",
    "a = 0\n",
    "for i in raw2:\n",
    "    words = i.lower()\n",
    "    words = words.split()\n",
    "    for word in words:\n",
    "        a += 1\n",
    "        counter[word] += 1\n",
    "        if word not in tokenizer:\n",
    "            tokenizer.append(word)\n",
    "len(tokenizer)\n",
    "tokenizer\n",
    "words = raw2.split()\n",
    "for word in words:\n",
    "    a += 1\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the negation tagging, put the negation until ^[.:;!?]$ (until the punctuation mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#out of the box tokenizer and counter\n",
    "from nltk.probability import FreqDist\n",
    "counter = FreqDist()\n",
    "tokens_nltk = word_tokenize(raw2)\n",
    "\n",
    "tokenizer = []\n",
    "for word in tokens_nltk:\n",
    "    counter[word.lower()] += 1\n",
    "\n",
    "len(tokens_nltk)\n",
    "len(counter)\n",
    "counter['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter.B() #is the number of unique words (?)\n",
    "#x = counter.N #is the number of words\n",
    "#counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials for regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD:\n",
    "### Check the paper that is mentioned in notion by UCLondon\n",
    "\n",
    "* Delete all the reviews that are not in english\n",
    "* Same number of negative that as positive reviews for the sets\n",
    "* Tokenization and BoW creation (**BoW with frequency or with presence?**)\n",
    "* unigrams and bigrams\n",
    "* lower the case\n",
    "* POS tagging?\n",
    "* EDA como en potts con las palabras mas frecuentes en positives and negatives reviews\n",
    "* Handling negation? _NOT or with sentiment negative scoring?\n",
    "* See the book by Bing Lui for how to identify fake news, resonates with anomaly detection and identifying if a review is fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123456789', '987654321', '987654321']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Been coming here since I was in grade 9 so about 10 years now (wow!) staff are very friendly and prices are ridiculously cheap. I remember back in my younger days being short on change and they never cared! Super nice family owned businesses. I always get the California roll, either I grab one out of the fridge or have them make it fresh if there's none. The tofu is also really good! They also sell so many different kinds of pop in a can for a buck and different Asian treats like pocky!\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\"Hello my Number is 123456789 and  \n",
    "             my friend's number is 987654321, and my number is also 987654321\"\"\"\n",
    "    \n",
    "# A sample regular expression to find digits.  \n",
    "regex = '\\d+'             \n",
    "    \n",
    "match = re.findall(regex, string)  \n",
    "print(match)\n",
    "raw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation tagging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesimist(text):\n",
    "    x = text\n",
    "    x = x.split()\n",
    "    hasta = 0\n",
    "    desde = 0\n",
    "    c = 0\n",
    "    j = 0\n",
    "    passer = False\n",
    "    for k in range(len(x)):\n",
    "        #print(j)\n",
    "        #print(k)\n",
    "        #if not passer:\n",
    "            #continue\n",
    "        passer = True\n",
    "\n",
    "        i = x[k]\n",
    "        negation_string = r\"\"\"\n",
    "        ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "            havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "            wont|wouldnt|but|doesnt|didnt|isnt|arent|aint\n",
    "        )$|n't\n",
    "        \"\"\"\n",
    "        #print(\"first\")\n",
    "        c +=1\n",
    "        #match = re.search(r'\\bthe\\b',i)\n",
    "        neg = re.compile(negation_string, re.VERBOSE | re.I | re.UNICODE)\n",
    "        match = neg.findall(i)\n",
    "        #print(i + \"i\")\n",
    "        #print('desde', desde)\n",
    "        #print(f'hasta menos desde es: {hasta - desde} y c es {c}')\n",
    "        if c < (hasta - desde):\n",
    "            continue\n",
    "        if match:\n",
    "            c = 0\n",
    "            desde = k + 1\n",
    "            #print(c - 1)\n",
    "            #j = i\n",
    "            jump = k\n",
    "            for j in range(jump, 100):\n",
    "\n",
    "                #print(j)\n",
    "                try:\n",
    "                    comma = re.search(r'[.:;!?]', x[j])\n",
    "                    #print(\"second\")\n",
    "                    if comma:\n",
    "                        hasta = j + 1\n",
    "                        c += 1\n",
    "                        \n",
    "                        #this try to avoid error if there is no punctuation error before the phrase ends\n",
    "                        try:\n",
    "                            for i in range(desde, hasta):\n",
    "                                repl = re.match(r'\\w+', x[i])\n",
    "                                x[i] = repl.group() + \"_NOT\"\n",
    "                            \n",
    "                            c = 0\n",
    "                            break\n",
    "                        except:\n",
    "                            #print(' '.join(x))\n",
    "                            c = 0\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "        if match:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "                #continue\n",
    "\n",
    "\n",
    "            #else:\n",
    "                #continue\n",
    "            #break\n",
    "    xx = ' '.join(x)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally the negation tagging is working, next steps:\n",
    "    1. Identify all the negation words on the regular expresion, can be taken from one paper.\n",
    "    2. Implement it i conjuction with the tokenizer and the stop words removal\n",
    "    3. run it for all the dataset\n",
    "    4. Balance the positive and negative classes on the data set that we are going to take to make all the trials.\n",
    "    5. Finish the identification of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current status\n",
    "\n",
    "order of the pipeline (in **bold** what is already done)\n",
    "\n",
    "(balance the sample 50/50 in reviews)\n",
    "1. **Negator**\n",
    "2. **Tokenizer**\n",
    "3. **Stop words removal (kind of done, have to figure it out)**\n",
    "3. **BoW**\n",
    "\n",
    "*Follows: Select the features and extend the functionality for all the reviews*\n",
    "1. bigram\n",
    "2. Positive Tokens\n",
    "3. Negative Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trial = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function of negation taggin to each row\n",
    "trial[\"sample\"] = trial.loc[:, \"text\"].apply(pesimist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Empanadas House</td>\n",
       "      <td>5</td>\n",
       "      <td>I ordered feta cheese and spinach empanadas an...</td>\n",
       "      <td>1</td>\n",
       "      <td>I ordered feta cheese and spinach empanadas an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>2</td>\n",
       "      <td>I've eaten at this location since 1997/98, so ...</td>\n",
       "      <td>0</td>\n",
       "      <td>I've eaten at this location since 1997/98, so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>5</td>\n",
       "      <td>Wetzels's Pretzels is definitely amazing and v...</td>\n",
       "      <td>1</td>\n",
       "      <td>Wetzels's Pretzels is definitely amazing and v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carl's Jr</td>\n",
       "      <td>1</td>\n",
       "      <td>WORST experience EVER!!!!! never have i ate an...</td>\n",
       "      <td>0</td>\n",
       "      <td>WORST experience EVER!!!!! never have_NOT i_NO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carl's Jr</td>\n",
       "      <td>4</td>\n",
       "      <td>Hot fresh food usually.  Staff seems to turn o...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hot fresh food usually. Staff seems to turn ov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  stars  \\\n",
       "0  The Empanadas House      5   \n",
       "1     Middle East Deli      2   \n",
       "2    Wetzel's Pretzels      5   \n",
       "3            Carl's Jr      1   \n",
       "4            Carl's Jr      4   \n",
       "\n",
       "                                                text  Sentiment  \\\n",
       "0  I ordered feta cheese and spinach empanadas an...          1   \n",
       "1  I've eaten at this location since 1997/98, so ...          0   \n",
       "2  Wetzels's Pretzels is definitely amazing and v...          1   \n",
       "3  WORST experience EVER!!!!! never have i ate an...          0   \n",
       "4  Hot fresh food usually.  Staff seems to turn o...          1   \n",
       "\n",
       "                                              sample  \n",
       "0  I ordered feta cheese and spinach empanadas an...  \n",
       "1  I've eaten at this location since 1997/98, so ...  \n",
       "2  Wetzels's Pretzels is definitely amazing and v...  \n",
       "3  WORST experience EVER!!!!! never have_NOT i_NO...  \n",
       "4  Hot fresh food usually. Staff seems to turn ov...  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.happyfuntokenizing import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preserve_case': False, 'all_in': False}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tok.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial[\"tokens\"] = trial.loc[:, \"sample\"].apply(tok.tokenize)\n",
    "trial.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    44431\n",
       "0    13695\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing 'filler' words\n",
    "Counting the most common words over all the corpora we can tell that the most common ones, and thus, tne ones that may play a bid role on the classification, are not meaningful for us to discover which words are really expressing a positive or negative emotion, thus, the first 15 words are going to be removed from the corpus.\n",
    "\n",
    "In the following cells, the most common words are shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = trial['tokens'].to_numpy()\n",
    "review1 = reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_words(raw_reviews, w_number):\n",
    "    \"\"\"\n",
    "    Creates a list with the most 'w_number' (number) of words on the whole corpus\n",
    "    ---------\n",
    "    raw_reviews : all the column of the raw reviews\n",
    "    w_number = number of most common words that wish to be extracted\n",
    "    \"\"\"\n",
    "    counter = FreqDist()\n",
    "    for review in raw_reviews:\n",
    "        for word in review:\n",
    "            counter[word.lower()] += 1\n",
    "    counting = counter.most_common(w_number)\n",
    "    commons = []\n",
    "    for word in counting:\n",
    "        commons.append(word[0])\n",
    "        if len(commons) == w_number:\n",
    "            break\n",
    "    return commons\n",
    "cc = most_common_words(reviews, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "Using the tokens, of course\n",
    "* Make the corpora\n",
    "* Make the vectors with word presence/frecuency. I think presenc may be better for vector-space representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD: check why is not working when the number of ommited words is omre than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1700"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corpus_builder(raw_reviews, most_common):\n",
    "    \"\"\"\n",
    "    Create the bag of words of all the words present on the reviews, ommiting the 'most_common' words as they are \n",
    "    conseidered as fillers with low influence on the classification\n",
    "    -----------\n",
    "    raw_reviews : all the column of the raw reviews\n",
    "    most_common : list of most common words that wish to be ommited\n",
    "    \"\"\"\n",
    "    main_corpus = []\n",
    "    for review in raw_reviews:\n",
    "        for word in review:\n",
    "            if word in most_common:\n",
    "                continue\n",
    "            if word not in main_corpus:\n",
    "                main_corpus.append(word)\n",
    "    return main_corpus\n",
    "bow = corpus_builder(reviews, cc)\n",
    "len(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1700"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorizer(bow, ind_review, ommitted_words):\n",
    "    \"\"\"\n",
    "    Vectorize the review based on the counting of the words present on the review, \n",
    "    the appereances are counted on a dictionary, the value of the words (keys) that are not present remain as '0'\n",
    "    ----------\n",
    "    bow : bag of all the words in list (unique values)\n",
    "    vector: individual review tokenized on the form of a list\n",
    "    \"\"\"\n",
    "    counter = dict.fromkeys(bow, 0)\n",
    "    for word in ind_review:\n",
    "        if word in ommitted_words:\n",
    "            continue\n",
    "        else:\n",
    "            counter[word] += 1\n",
    "    return list(counter.values())\n",
    "vv = vectorizer(bow, review1, cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_builder(bow, review_series, ommited_words):\n",
    "    \"\"\"\n",
    "    Creates the matrix of features based on the term frecuency vectors created by the function vectorizer\n",
    "    ----------\n",
    "    review_series : pandas object (series)  i.e dataframe[name_of_column]\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    i = 0\n",
    "    \n",
    "    for review in review_series:\n",
    "        word_vector = vectorizer(bow, review, ommited_words)\n",
    "        if i < 1:\n",
    "            X = np.array([word_vector])\n",
    "            i += 1\n",
    "            continue\n",
    "        X = np.append(X, [word_vector], axis = 0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for the creation of the feature matrix\n",
    "* features thus far: \n",
    "    * word frecuency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 1700)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = trial['tokens'].to_numpy()\n",
    "common = most_common_words(reviews, 9)\n",
    "bow = corpus_builder(reviews, most_common_in_corpus)\n",
    "raw_reviews = trial['tokens']\n",
    "X = matrix_builder(bow, raw_reviews, common)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'i', 'a', 'to', 'was', 'of', 'in', 'is', 'it']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words(reviews, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1694"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 323),\n",
       " ('and', 201),\n",
       " ('i', 179),\n",
       " ('a', 131),\n",
       " ('to', 130),\n",
       " ('was', 106),\n",
       " ('of', 90),\n",
       " ('in', 81),\n",
       " ('is', 71),\n",
       " ('it', 66),\n",
       " ('this', 62),\n",
       " ('for', 61),\n",
       " ('we', 58),\n",
       " ('my', 54),\n",
       " ('but', 49),\n",
       " ('that', 48),\n",
       " ('place', 46),\n",
       " ('food', 43),\n",
       " ('they', 42),\n",
       " ('on', 42),\n",
       " ('with', 36),\n",
       " ('not', 36),\n",
       " ('at', 35),\n",
       " ('have', 34),\n",
       " ('are', 33)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FreqDist in module nltk.probability:\n",
      "\n",
      "class FreqDist(collections.Counter)\n",
      " |  FreqDist(samples=None)\n",
      " |  \n",
      " |  A frequency distribution for the outcomes of an experiment.  A\n",
      " |  frequency distribution records the number of times each outcome of\n",
      " |  an experiment has occurred.  For example, a frequency distribution\n",
      " |  could be used to record the frequency of each word type in a\n",
      " |  document.  Formally, a frequency distribution can be defined as a\n",
      " |  function mapping from each sample to the number of times that\n",
      " |  sample occurred as an outcome.\n",
      " |  \n",
      " |  Frequency distributions are generally constructed by running a\n",
      " |  number of experiments, and incrementing the count for a sample\n",
      " |  every time it is an outcome of an experiment.  For example, the\n",
      " |  following code will produce a frequency distribution that encodes\n",
      " |  how often each word occurs in a text:\n",
      " |  \n",
      " |      >>> from nltk.tokenize import word_tokenize\n",
      " |      >>> from nltk.probability import FreqDist\n",
      " |      >>> sent = 'This is an example sentence'\n",
      " |      >>> fdist = FreqDist()\n",
      " |      >>> for word in word_tokenize(sent):\n",
      " |      ...    fdist[word.lower()] += 1\n",
      " |  \n",
      " |  An equivalent way to do this is with the initializer:\n",
      " |  \n",
      " |      >>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FreqDist\n",
      " |      collections.Counter\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  B(self)\n",
      " |      Return the total number of sample values (or \"bins\") that\n",
      " |      have counts greater than zero.  For the total\n",
      " |      number of sample outcomes recorded, use ``FreqDist.N()``.\n",
      " |      (FreqDist.B() is the same as len(FreqDist).)\n",
      " |      \n",
      " |      :rtype: int\n",
      " |  \n",
      " |  N(self)\n",
      " |      Return the total number of sample outcomes that have been\n",
      " |      recorded by this FreqDist.  For the number of unique\n",
      " |      sample values (or bins) with counts greater than zero, use\n",
      " |      ``FreqDist.B()``.\n",
      " |      \n",
      " |      :rtype: int\n",
      " |  \n",
      " |  Nr(self, r, bins=None)\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Add counts from two counters.\n",
      " |      \n",
      " |      >>> FreqDist('abbb') + FreqDist('bcc')\n",
      " |      FreqDist({'b': 4, 'c': 2, 'a': 1})\n",
      " |  \n",
      " |  __and__(self, other)\n",
      " |      Intersection is the minimum of corresponding counts.\n",
      " |      \n",
      " |      >>> FreqDist('abbb') & FreqDist('bcc')\n",
      " |      FreqDist({'b': 1})\n",
      " |  \n",
      " |  __delitem__(self, key)\n",
      " |      Override ``Counter.__delitem__()`` to invalidate the cached N\n",
      " |  \n",
      " |  __ge__ lambda self, other\n",
      " |      # @total_ordering doesn't work here, since the class inherits from a builtin class\n",
      " |  \n",
      " |  __gt__ lambda self, other\n",
      " |  \n",
      " |  __init__(self, samples=None)\n",
      " |      Construct a new frequency distribution.  If ``samples`` is\n",
      " |      given, then the frequency distribution will be initialized\n",
      " |      with the count of each object in ``samples``; otherwise, it\n",
      " |      will be initialized to be empty.\n",
      " |      \n",
      " |      In particular, ``FreqDist()`` returns an empty frequency\n",
      " |      distribution; and ``FreqDist(samples)`` first creates an empty\n",
      " |      frequency distribution, and then calls ``update`` with the\n",
      " |      list ``samples``.\n",
      " |      \n",
      " |      :param samples: The samples to initialize the frequency\n",
      " |          distribution with.\n",
      " |      :type samples: Sequence\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__ lambda self, other\n",
      " |  \n",
      " |  __or__(self, other)\n",
      " |      Union is the maximum of value in either of the input counters.\n",
      " |      \n",
      " |      >>> FreqDist('abbb') | FreqDist('bcc')\n",
      " |      FreqDist({'b': 3, 'c': 2, 'a': 1})\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a string representation of this FreqDist.\n",
      " |      \n",
      " |      :rtype: string\n",
      " |  \n",
      " |  __setitem__(self, key, val)\n",
      " |      Override ``Counter.__setitem__()`` to invalidate the cached N\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return a string representation of this FreqDist.\n",
      " |      \n",
      " |      :rtype: string\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |      Subtract count, but keep only results with positive counts.\n",
      " |      \n",
      " |      >>> FreqDist('abbbc') - FreqDist('bccd')\n",
      " |      FreqDist({'b': 2, 'a': 1})\n",
      " |  \n",
      " |  __unicode__ = __str__(self)\n",
      " |  \n",
      " |  copy(self)\n",
      " |      Create a copy of this frequency distribution.\n",
      " |      \n",
      " |      :rtype: FreqDist\n",
      " |  \n",
      " |  freq(self, sample)\n",
      " |      Return the frequency of a given sample.  The frequency of a\n",
      " |      sample is defined as the count of that sample divided by the\n",
      " |      total number of sample outcomes that have been recorded by\n",
      " |      this FreqDist.  The count of a sample is defined as the\n",
      " |      number of times that sample outcome was recorded by this\n",
      " |      FreqDist.  Frequencies are always real numbers in the range\n",
      " |      [0, 1].\n",
      " |      \n",
      " |      :param sample: the sample whose frequency\n",
      " |             should be returned.\n",
      " |      :type sample: any\n",
      " |      :rtype: float\n",
      " |  \n",
      " |  hapaxes(self)\n",
      " |      Return a list of all samples that occur once (hapax legomena)\n",
      " |      \n",
      " |      :rtype: list\n",
      " |  \n",
      " |  max(self)\n",
      " |      Return the sample with the greatest number of outcomes in this\n",
      " |      frequency distribution.  If two or more samples have the same\n",
      " |      number of outcomes, return one of them; which sample is\n",
      " |      returned is undefined.  If no outcomes have occurred in this\n",
      " |      frequency distribution, return None.\n",
      " |      \n",
      " |      :return: The sample with the maximum number of outcomes in this\n",
      " |              frequency distribution.\n",
      " |      :rtype: any or None\n",
      " |  \n",
      " |  pformat(self, maxlen=10)\n",
      " |      Return a string representation of this FreqDist.\n",
      " |      \n",
      " |      :param maxlen: The maximum number of items to display\n",
      " |      :type maxlen: int\n",
      " |      :rtype: string\n",
      " |  \n",
      " |  plot(self, *args, **kwargs)\n",
      " |      Plot samples from the frequency distribution\n",
      " |      displaying the most frequent sample first.  If an integer\n",
      " |      parameter is supplied, stop after this many samples have been\n",
      " |      plotted.  For a cumulative plot, specify cumulative=True.\n",
      " |      (Requires Matplotlib to be installed.)\n",
      " |      \n",
      " |      :param title: The title for the graph\n",
      " |      :type title: str\n",
      " |      :param cumulative: A flag to specify whether the plot is cumulative (default = False)\n",
      " |      :type title: bool\n",
      " |  \n",
      " |  pprint(self, maxlen=10, stream=None)\n",
      " |      Print a string representation of this FreqDist to 'stream'\n",
      " |      \n",
      " |      :param maxlen: The maximum number of items to print\n",
      " |      :type maxlen: int\n",
      " |      :param stream: The stream to print to. stdout by default\n",
      " |  \n",
      " |  r_Nr(self, bins=None)\n",
      " |      Return the dictionary mapping r to Nr, the number of samples with frequency r, where Nr > 0.\n",
      " |      \n",
      " |      :type bins: int\n",
      " |      :param bins: The number of possible sample outcomes.  ``bins``\n",
      " |          is used to calculate Nr(0).  In particular, Nr(0) is\n",
      " |          ``bins-self.B()``.  If ``bins`` is not specified, it\n",
      " |          defaults to ``self.B()`` (so Nr(0) will be 0).\n",
      " |      :rtype: int\n",
      " |  \n",
      " |  setdefault(self, key, val)\n",
      " |      Override ``Counter.setdefault()`` to invalidate the cached N\n",
      " |  \n",
      " |  tabulate(self, *args, **kwargs)\n",
      " |      Tabulate the given samples from the frequency distribution (cumulative),\n",
      " |      displaying the most frequent sample first.  If an integer\n",
      " |      parameter is supplied, stop after this many samples have been\n",
      " |      plotted.\n",
      " |      \n",
      " |      :param samples: The samples to plot (default is all samples)\n",
      " |      :type samples: list\n",
      " |      :param cumulative: A flag to specify whether the freqs are cumulative (default = False)\n",
      " |      :type title: bool\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  update(self, *args, **kwargs)\n",
      " |      Override ``Counter.update()`` to invalidate the cached N\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.Counter:\n",
      " |  \n",
      " |  __iadd__(self, other)\n",
      " |      Inplace add from another counter, keeping only positive counts.\n",
      " |      \n",
      " |      >>> c = Counter('abbb')\n",
      " |      >>> c += Counter('bcc')\n",
      " |      >>> c\n",
      " |      Counter({'b': 4, 'c': 2, 'a': 1})\n",
      " |  \n",
      " |  __iand__(self, other)\n",
      " |      Inplace intersection is the minimum of corresponding counts.\n",
      " |      \n",
      " |      >>> c = Counter('abbb')\n",
      " |      >>> c &= Counter('bcc')\n",
      " |      >>> c\n",
      " |      Counter({'b': 1})\n",
      " |  \n",
      " |  __ior__(self, other)\n",
      " |      Inplace union is the maximum of value from either counter.\n",
      " |      \n",
      " |      >>> c = Counter('abbb')\n",
      " |      >>> c |= Counter('bcc')\n",
      " |      >>> c\n",
      " |      Counter({'b': 3, 'c': 2, 'a': 1})\n",
      " |  \n",
      " |  __isub__(self, other)\n",
      " |      Inplace subtract counter, but keep only results with positive counts.\n",
      " |      \n",
      " |      >>> c = Counter('abbbc')\n",
      " |      >>> c -= Counter('bccd')\n",
      " |      >>> c\n",
      " |      Counter({'b': 2, 'a': 1})\n",
      " |  \n",
      " |  __missing__(self, key)\n",
      " |      The count of elements not in the Counter is zero.\n",
      " |  \n",
      " |  __neg__(self)\n",
      " |      Subtracts from an empty counter.  Strips positive and zero counts,\n",
      " |      and flips the sign on negative counts.\n",
      " |  \n",
      " |  __pos__(self)\n",
      " |      Adds an empty counter, effectively stripping negative and zero counts\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  elements(self)\n",
      " |      Iterator over elements repeating each as many times as its count.\n",
      " |      \n",
      " |      >>> c = Counter('ABCABC')\n",
      " |      >>> sorted(c.elements())\n",
      " |      ['A', 'A', 'B', 'B', 'C', 'C']\n",
      " |      \n",
      " |      # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n",
      " |      >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n",
      " |      >>> product = 1\n",
      " |      >>> for factor in prime_factors.elements():     # loop over factors\n",
      " |      ...     product *= factor                       # and multiply them\n",
      " |      >>> product\n",
      " |      1836\n",
      " |      \n",
      " |      Note, if an element's count has been set to zero or is a negative\n",
      " |      number, elements() will ignore it.\n",
      " |  \n",
      " |  most_common(self, n=None)\n",
      " |      List the n most common elements and their counts from the most\n",
      " |      common to the least.  If n is None, then list all element counts.\n",
      " |      \n",
      " |      >>> Counter('abcdeabcdabcaba').most_common(3)\n",
      " |      [('a', 5), ('b', 4), ('c', 3)]\n",
      " |  \n",
      " |  subtract(*args, **kwds)\n",
      " |      Like dict.update() but subtracts counts instead of replacing them.\n",
      " |      Counts can be reduced below zero.  Both the inputs and outputs are\n",
      " |      allowed to contain zero and negative counts.\n",
      " |      \n",
      " |      Source can be an iterable, a dictionary, or another Counter instance.\n",
      " |      \n",
      " |      >>> c = Counter('which')\n",
      " |      >>> c.subtract('witch')             # subtract elements from another iterable\n",
      " |      >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n",
      " |      >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n",
      " |      0\n",
      " |      >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n",
      " |      -1\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.Counter:\n",
      " |  \n",
      " |  fromkeys(iterable, v=None) from builtins.type\n",
      " |      Create a new dictionary with keys from iterable and values set to value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from collections.Counter:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(FreqDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-d76cbfc21766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "counter.plot(samples=counter.most_common(20).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
