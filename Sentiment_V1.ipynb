{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the 'business' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_json(\"yelp_academic_dataset_business.json\", chunksize = 1000, lines = True)\n",
    "drop_cols = ['address', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open','attributes', 'hours']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deleting non useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "chunks = []\n",
    "a = 0\n",
    "for chunk in df2:\n",
    "    a += 1\n",
    "    chunk_b = chunk.drop(drop_cols, axis = 1)\n",
    "    restas = chunk_b[chunk_b['categories'].str.contains('restaurant', case = False, na = False)]\n",
    "    chunks.append(restas)\n",
    "restaurants = pd.concat(chunks, ignore_index= True, join='outer')\n",
    "end = time.time()\n",
    "elapsed = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63961, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the reviews dataset\n",
    "* Remember we made the merge to use ONLY restaurants data, because there were data from other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_raw = pd.read_json(\"yelp_academic_dataset_review.json\", chunksize=100000, lines = True)\n",
    "drop_cols = ['review_id', 'user_id','useful', 'funny', 'cool', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using merge instead of join because we want to join in another column other than the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for chunk in reviews_raw:\n",
    "    a += 1\n",
    "    reviews = chunk.drop(drop_cols, axis = 1)\n",
    "    data = restaurants.merge(reviews, left_on = 'business_id', right_on = 'business_id',how = 'inner')\n",
    "    if a == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally the data to be preprocessed (the \"text\" column, to be exact) \n",
    "TBD:\n",
    "* Delete all number 3 i.e neutral \n",
    "* Same number of positive as negatives\n",
    "* Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>categories</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pQeaRpvuhoEqudo3uymHIQ</td>\n",
       "      <td>The Empanadas House</td>\n",
       "      <td>Champaign</td>\n",
       "      <td>Ethnic Food, Food Trucks, Specialty Food, Impo...</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the empanadas from the Empanadas House!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CsLQLiRoafpJPJSkNX2h5Q</td>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Food, Restaurants, Grocery, Middle Eastern</td>\n",
       "      <td>3</td>\n",
       "      <td>Definitely under new management, and the dinin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CsLQLiRoafpJPJSkNX2h5Q</td>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Food, Restaurants, Grocery, Middle Eastern</td>\n",
       "      <td>3</td>\n",
       "      <td>I will also agree that this place has great fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lu7vtrp_bE9PnxWfA8g4Pg</td>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>Thornhill</td>\n",
       "      <td>Japanese, Fast Food, Food Court, Restaurants</td>\n",
       "      <td>4</td>\n",
       "      <td>Been coming here since I was in grade 9 so abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vjTVxnsQEZ34XjYNS-XUpA</td>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Food, Pretzels, Bakeries, Fast Food, Restaurants</td>\n",
       "      <td>5</td>\n",
       "      <td>Love Wetzel's pretzels! I always get them when...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 name       city  \\\n",
       "0  pQeaRpvuhoEqudo3uymHIQ  The Empanadas House  Champaign   \n",
       "1  CsLQLiRoafpJPJSkNX2h5Q     Middle East Deli  Charlotte   \n",
       "2  CsLQLiRoafpJPJSkNX2h5Q     Middle East Deli  Charlotte   \n",
       "3  lu7vtrp_bE9PnxWfA8g4Pg         Banzai Sushi  Thornhill   \n",
       "4  vjTVxnsQEZ34XjYNS-XUpA    Wetzel's Pretzels    Phoenix   \n",
       "\n",
       "                                          categories  stars  \\\n",
       "0  Ethnic Food, Food Trucks, Specialty Food, Impo...      5   \n",
       "1         Food, Restaurants, Grocery, Middle Eastern      3   \n",
       "2         Food, Restaurants, Grocery, Middle Eastern      3   \n",
       "3       Japanese, Fast Food, Food Court, Restaurants      4   \n",
       "4   Food, Pretzels, Bakeries, Fast Food, Restaurants      5   \n",
       "\n",
       "                                                text  \n",
       "0  I love the empanadas from the Empanadas House!...  \n",
       "1  Definitely under new management, and the dinin...  \n",
       "2  I will also agree that this place has great fo...  \n",
       "3  Been coming here since I was in grade 9 so abo...  \n",
       "4  Love Wetzel's pretzels! I always get them when...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a toyset to work in trials from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Empanadas House</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the empanadas from the Empanadas House!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>3</td>\n",
       "      <td>Definitely under new management, and the dinin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>3</td>\n",
       "      <td>I will also agree that this place has great fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>4</td>\n",
       "      <td>Been coming here since I was in grade 9 so abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>5</td>\n",
       "      <td>Love Wetzel's pretzels! I always get them when...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  stars  \\\n",
       "0  The Empanadas House      5   \n",
       "1     Middle East Deli      3   \n",
       "2     Middle East Deli      3   \n",
       "3         Banzai Sushi      4   \n",
       "4    Wetzel's Pretzels      5   \n",
       "\n",
       "                                                text  \n",
       "0  I love the empanadas from the Empanadas House!...  \n",
       "1  Definitely under new management, and the dinin...  \n",
       "2  I will also agree that this place has great fo...  \n",
       "3  Been coming here since I was in grade 9 so abo...  \n",
       "4  Love Wetzel's pretzels! I always get them when...  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data.loc[0:99, ['name', 'stars', 'text']]\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some graphs to know the number of reviews by ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a201d2fd0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bars = data3['stars'].value_counts()\n",
    "sns.barplot(x = bars.index, y = bars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Delete the number 3's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 3)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.drop(data2[data2['stars'] == 3].index, inplace = True)\n",
    "data2 = data2.reset_index(drop = True)\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We want just the reviews with 4-5 to be positive and the 1-2 to be negative, we do that on the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['Sentiment'] = data2['stars'].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use \"3\" as negative to augment the data and  balance the classes\n",
    "One could argue that the reviews that have not unconditionally positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data2.copy()\n",
    "neg_half = list((data3[data3['stars'] <=3]).index)\n",
    "pos_half = list(set(range(data3.shape[0])) - set(neg_half))\n",
    "pos_half = pos_half[0:len(neg_half)]\n",
    "pos_half.extend(neg_half)\n",
    "dropper = list(set(range(data3.shape[0])) - set(pos_half))\n",
    "data3.drop(index = dropper, inplace = True)\n",
    "data3['Sentiment'] = data3['stars'].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3['Sentiment'].value_counts()\n",
    "data2 = data3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here start the NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to make a bag of words, it can be done manually, but also with sklearn.\n",
    "\n",
    "**Steps:** I am trying to extract the nest tokens with the tokenizer from Potts and then feed that already \"clean tokens\" to the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = data2['text'][1:3]\n",
    "raw\n",
    "raw2 = data2['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "counter = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = []\n",
    "a = 0\n",
    "for i in raw2:\n",
    "    words = i.lower()\n",
    "    words = words.split()\n",
    "    for word in words:\n",
    "        a += 1\n",
    "        counter[word] += 1\n",
    "        if word not in tokenizer:\n",
    "            tokenizer.append(word)\n",
    "len(tokenizer)\n",
    "tokenizer\n",
    "words = raw2.split()\n",
    "for word in words:\n",
    "    a += 1\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the negation tagging, put the negation until ^[.:;!?]$ (until the punctuation mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#out of the box tokenizer and counter\n",
    "from nltk.probability import FreqDist\n",
    "counter = FreqDist()\n",
    "tokens_nltk = word_tokenize(raw2)\n",
    "\n",
    "tokenizer = []\n",
    "for word in tokens_nltk:\n",
    "    counter[word.lower()] += 1\n",
    "\n",
    "len(tokens_nltk)\n",
    "len(counter)\n",
    "counter['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 12),\n",
       " ('.', 6),\n",
       " ('and', 5),\n",
       " ('the', 5),\n",
       " ('has', 4),\n",
       " ('a', 3),\n",
       " ('that', 3),\n",
       " (\"'s\", 2),\n",
       " ('some', 2),\n",
       " ('have', 2),\n",
       " ('on', 2),\n",
       " ('definitely', 1),\n",
       " ('under', 1),\n",
       " ('new', 1),\n",
       " ('management', 1),\n",
       " ('dining', 1),\n",
       " ('area', 1),\n",
       " ('been', 1),\n",
       " ('totally', 1),\n",
       " ('redone', 1)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.B() #is the number of unique words (?)\n",
    "x = counter.N #is the number of words\n",
    "counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD:\n",
    "### Check the paper that is mentioned in notion by UCLondon\n",
    "\n",
    "* Delete all the reviews that are not in english\n",
    "* Same number of negative that as positive reviews for the sets\n",
    "* Tokenization and BoW creation (**BoW with frequency or with presence?**)\n",
    "* unigrams and bigrams\n",
    "* lower the case\n",
    "* POS tagging?\n",
    "* EDA como en potts con las palabras mas frecuentes en positives and negatives reviews\n",
    "* Handling negation? _NOT or with sentiment negative scoring?\n",
    "* See the book by Bing Lui for how to identify fake news, resonates with anomaly detection and identifying if a review is fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123456789', '987654321', '987654321']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Definitely under new management, and the dining area has been totally redone. Big, comfy chairs, dangling bistro lighting, it's somewhat comfier.\\n\\nThe menu has changed quite a bit - this learns more towards Turkish than anything else, and has some interesting options for you meaters. Veg fare hasn't budged, and neither have the prices, but the quality and portion size have improved some. Falafel now comes with a smidge of hummus on the side, which is a nice touch.\\n\\nEveryone that waited on me was very tall, female, and beautiful, in that awesome Mediterranean way, so there's that.\""
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\"Hello my Number is 123456789 and  \n",
    "             my friend's number is 987654321, and my number is also 987654321\"\"\"\n",
    "    \n",
    "# A sample regular expression to find digits.  \n",
    "regex = '\\d+'             \n",
    "    \n",
    "match = re.findall(regex, string)  \n",
    "print(match)\n",
    "raw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation tagging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesimist(text):\n",
    "    x = text\n",
    "    x = x.split()\n",
    "    hasta = 0\n",
    "    desde = 0\n",
    "    c = 0\n",
    "    j = 0\n",
    "    passer = False\n",
    "    for k in range(len(x)):\n",
    "        #print(j)\n",
    "        #print(k)\n",
    "        #if not passer:\n",
    "            #continue\n",
    "        passer = True\n",
    "\n",
    "        i = x[k]\n",
    "        negation_string = r\"\"\"\n",
    "        ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "            havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "            wont|wouldnt|but|doesnt|didnt|isnt|arent|aint\n",
    "        )$|n't\n",
    "        \"\"\"\n",
    "        #print(\"first\")\n",
    "        c +=1\n",
    "        #match = re.search(r'\\bthe\\b',i)\n",
    "        neg = re.compile(negation_string, re.VERBOSE | re.I | re.UNICODE)\n",
    "        match = neg.findall(i)\n",
    "        #print(i + \"i\")\n",
    "        #print('desde', desde)\n",
    "        #print(f'hasta menos desde es: {hasta - desde} y c es {c}')\n",
    "        if c < (hasta - desde):\n",
    "            continue\n",
    "        if match:\n",
    "            c = 0\n",
    "            desde = k + 1\n",
    "            #print(c - 1)\n",
    "            #j = i\n",
    "            jump = k\n",
    "            for j in range(jump, 100):\n",
    "\n",
    "                #print(j)\n",
    "                try:\n",
    "                    comma = re.search(r'[.:;!?]', x[j])\n",
    "                    #print(\"second\")\n",
    "                    if comma:\n",
    "                        hasta = j + 1\n",
    "                        c += 1\n",
    "                        \n",
    "                        #this try to avoid error if there is no punctuation error before the phrase ends\n",
    "                        try:\n",
    "                            for i in range(desde, hasta):\n",
    "                                repl = re.match(r'\\w+', x[i])\n",
    "                                x[i] = repl.group() + \"_NOT\"\n",
    "                            \n",
    "                            c = 0\n",
    "                            break\n",
    "                        except:\n",
    "                            #print(' '.join(x))\n",
    "                            c = 0\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "        if match:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "                #continue\n",
    "\n",
    "\n",
    "            #else:\n",
    "                #continue\n",
    "            #break\n",
    "    xx = ' '.join(x)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally the negation tagging is working, next steps:\n",
    "    1. Identify all the negation words on the regular expresion, can be taken from one paper.\n",
    "    2. Implement it i conjuction with the tokenizer and the stop words removal\n",
    "    3. run it for all the dataset\n",
    "    4. Balance the positive and negative classes on the data set that we are going to take to make all the trials.\n",
    "    5. Finish the identification of features\n",
    "    6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current status\n",
    "\n",
    "order of the pipeline (in **bold** what is already done)\n",
    "\n",
    "(balance the sample 50/50 in reviews)\n",
    "1. **Negator**\n",
    "2. **Tokenizer**\n",
    "3. Stop words removal (kind of done, have to figure it out)\n",
    "3. BoW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trial = data2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function of negation taggin to each row\n",
    "trial[\"sample\"] = trial.loc[:, \"text\"].apply(pesimist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Empanadas House</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the empanadas from the Empanadas House!...</td>\n",
       "      <td>1</td>\n",
       "      <td>I love the empanadas from the Empanadas House!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>3</td>\n",
       "      <td>Definitely under new management, and the dinin...</td>\n",
       "      <td>0</td>\n",
       "      <td>Definitely under new management, and the dinin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>3</td>\n",
       "      <td>I will also agree that this place has great fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>I will also agree that this place has great fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>4</td>\n",
       "      <td>Been coming here since I was in grade 9 so abo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Been coming here since I was in grade 9 so abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>5</td>\n",
       "      <td>Love Wetzel's pretzels! I always get them when...</td>\n",
       "      <td>1</td>\n",
       "      <td>Love Wetzel's pretzels! I always get them when...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  stars  \\\n",
       "0  The Empanadas House      5   \n",
       "1     Middle East Deli      3   \n",
       "2     Middle East Deli      3   \n",
       "3         Banzai Sushi      4   \n",
       "4    Wetzel's Pretzels      5   \n",
       "\n",
       "                                                text  Sentiment  \\\n",
       "0  I love the empanadas from the Empanadas House!...          1   \n",
       "1  Definitely under new management, and the dinin...          0   \n",
       "2  I will also agree that this place has great fo...          0   \n",
       "3  Been coming here since I was in grade 9 so abo...          1   \n",
       "4  Love Wetzel's pretzels! I always get them when...          1   \n",
       "\n",
       "                                              sample  \n",
       "0  I love the empanadas from the Empanadas House!...  \n",
       "1  Definitely under new management, and the dinin...  \n",
       "2  I will also agree that this place has great fo...  \n",
       "3  Been coming here since I was in grade 9 so abo...  \n",
       "4  Love Wetzel's pretzels! I always get them when...  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.happyfuntokenizing import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tok.tokenize(raw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['definitely',\n",
       " 'under',\n",
       " 'new',\n",
       " 'management',\n",
       " 'and',\n",
       " 'the',\n",
       " 'dining',\n",
       " 'area',\n",
       " 'has',\n",
       " 'been',\n",
       " 'totally',\n",
       " 'redone',\n",
       " 'big',\n",
       " 'comfy',\n",
       " 'chairs',\n",
       " 'dangling',\n",
       " 'bistro',\n",
       " 'lighting',\n",
       " \"it's\",\n",
       " 'somewhat',\n",
       " 'comfier',\n",
       " 'the',\n",
       " 'menu',\n",
       " 'has',\n",
       " 'changed',\n",
       " 'quite',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'this',\n",
       " 'learns',\n",
       " 'more',\n",
       " 'towards',\n",
       " 'turkish',\n",
       " 'than',\n",
       " 'anything',\n",
       " 'else',\n",
       " 'and',\n",
       " 'has',\n",
       " 'some',\n",
       " 'interesting',\n",
       " 'options',\n",
       " 'for',\n",
       " 'you',\n",
       " 'meaters',\n",
       " 'veg',\n",
       " 'fare',\n",
       " \"hasn't\",\n",
       " 'budged',\n",
       " 'and',\n",
       " 'neither',\n",
       " 'have',\n",
       " 'the',\n",
       " 'prices',\n",
       " 'but',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'and',\n",
       " 'portion',\n",
       " 'size',\n",
       " 'have',\n",
       " 'improved',\n",
       " 'some',\n",
       " 'falafel',\n",
       " 'now',\n",
       " 'comes',\n",
       " 'with',\n",
       " 'a',\n",
       " 'smidge',\n",
       " 'of',\n",
       " 'hummus',\n",
       " 'on',\n",
       " 'the',\n",
       " 'side',\n",
       " 'which',\n",
       " 'is',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'touch',\n",
       " 'everyone',\n",
       " 'that',\n",
       " 'waited',\n",
       " 'on',\n",
       " 'me',\n",
       " 'was',\n",
       " 'very',\n",
       " 'tall',\n",
       " 'female',\n",
       " 'and',\n",
       " 'beautiful',\n",
       " 'in',\n",
       " 'that',\n",
       " 'awesome',\n",
       " 'mediterranean',\n",
       " 'way',\n",
       " 'so',\n",
       " \"there's\",\n",
       " 'that']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preserve_case': False, 'all_in': False}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tok.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial[\"tokens\"] = trial.loc[:, \"sample\"].apply(tok.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'place', 'used', 'to', 'be', 'called', 'chicken', 'bonz', 'as', 'they', 'specialized', 'in', 'chicken', 'wings', 'and', 'had', 'a', 'few', 'other', 'items', 'on', 'the', 'side', 'they', 'were', 'pretty', 'good', 'and', 'i', 'ate', 'there', 'all', 'the', 'time', 'used', 'to', 'being', 'the', 'important', 'part', 'a', 'few', 'months', 'ago', 'they', 'broke', 'from', 'the', 'franchise', 'and', 'renamed', 'meat', 'chix', 'and', 'wieners', 'really', 'who', 'calls', 'a', 'restaurant', 'this', 'i', 'tried', 'it', 'out', 'a', 'few', 'times', 'after', 'the', 'change', 'but', 'today_not', 'was_not', 'the_not', 'last_not', 'straw_not', 'we', 'walked', 'in', 'and', 'ordered', 'the', 'usual', '6', 'wings', 'medium', 'for', 'me', 'same', 'for', 'my', 'wife', 'ranch', 'and', 'blue', 'cheese', 'dip', 'large', 'fries', 'to', 'share', 'and', 'two', 'drinks', 'well', 'they', 'changed', 'their', 'menu', 'and', 'no', 'longer', 'have', 'medium', 'sauce', 'or', 'blue', 'cheese', 'what', 'kind', 'of', 'wing', 'place', \"doesn't\", 'have', 'blue', 'cheese', 'my', 'wife', 'changes', 'her', 'order', 'to', 'a', 'portabello', 'burger', 'with', 'onion', 'rings', 'hot', 'sauce', 'on', 'the', 'side', 'almost', '25', 'dollars', 'later', 'we', 'sit', 'down', 'to', 'wait', 'and', 'wait', '...', 'and', 'wait', 'some', 'more', '25', 'minutes', 'later', 'we', 'get', 'our', 'order', 'did', 'i', 'mention', 'this', 'is', 'a', 'cook', 'to', 'order', 'fast-food', 'place', 'orders', 'used', 'to', 'come', 'out', 'at', 'lightning', 'speed', 'sometimes', 'before', 'we', 'had', 'even', 'filled', 'our', 'drinks', 'and', 'sat', 'down', 'not', 'anymore', 'we', 'were', 'assured', 'several', 'times', 'that', 'our', 'food', 'would', 'be', 'right', 'out', 'by', 'who', 'we', 'think', 'was', 'the', 'manager', 'however', 'she', 'grabbed', 'her', 'bags', 'from', 'the', 'office', 'that', 'was', 'set', 'up', 'in', 'a', 'booth', 'in', 'the', 'corner', 'of', 'the', 'restaurant', 'before', 'we', 'got', 'our', 'food', 'after', 'all', 'that', 'waiting', 'you', 'think', 'they', 'could', 'at', 'least', 'get', 'the', 'order', 'correct', 'right', 'nope', 'my', 'wife', 'got', 'her', 'burger', 'with', 'their', 'trendy', 'version', 'of', 'french', 'fries', 'that', 'look', 'like', 'little', 'more', 'than', 'thick', 'potato', 'chips', 'no', 'hot', 'sauce', 'no', 'onion', 'rings', 'i', 'get', 'my', '5', '1/2', 'wings', 'one', 'was', 'a', 'mutant', 'or', 'from', 'a', 'baby', 'chicken', 'ranch', 'and', 'no', 'fries', 'she', 'goes', 'to', 'cashier', 'the', 'only', 'person', 'no', 'working', 'outside', 'of', 'the', 'kitchen', 'btw', 'and', 'let', 'her', 'know', 'a', 'few', 'minutes', 'later', 'we', 'get', 'the', 'onion', 'rings', 'but', 'no', 'sauce', 'or', 'apology', 'the', 'food', 'itself', 'was', 'ok', 'but', 'again', 'used', 'to', 'be', 'much', 'better', 'the', 'wings', 'were', 'small', 'and', 'thinly', 'sauced', 'the', 'order', 'of', 'fries', 'was', 'small', 'and', 'not', 'particularly', 'good', 'my', 'wife', 'did', 'say', 'the', 'portabello', 'burger', 'was', 'good', 'i', 'really', 'wish', 'i', \"didn't\", 'have', 'to', 'write', 'this', 'since', 'the', 'previous', 'establishment', 'had', 'been', 'so', 'good', 'and', 'we', 'went', 'there', 'often', 'whatever', 'changed', 'in', 'the', 'past', 'few', 'months', 'has', 'certainly', 'been', 'a', 'bad', 'move', 'on', 'meat', 'chix', 'and', \"wiener's\", 'part', 'and', 'i', \"don't\", 'think', \"they'll\", 'be', 'seeing', 'us', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(trial.tokens[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trial.tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sample</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Empanadas House</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the empanadas from the Empanadas House!...</td>\n",
       "      <td>1</td>\n",
       "      <td>I love the empanadas from the Empanadas House!...</td>\n",
       "      <td>[i, love, the, empanadas, from, the, empanadas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>3</td>\n",
       "      <td>Definitely under new management, and the dinin...</td>\n",
       "      <td>0</td>\n",
       "      <td>Definitely under new management, and the dinin...</td>\n",
       "      <td>[definitely, under, new, management, and, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>3</td>\n",
       "      <td>I will also agree that this place has great fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>I will also agree that this place has great fo...</td>\n",
       "      <td>[i, will, also, agree, that, this, place, has,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>4</td>\n",
       "      <td>Been coming here since I was in grade 9 so abo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Been coming here since I was in grade 9 so abo...</td>\n",
       "      <td>[been, coming, here, since, i, was, in, grade,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>5</td>\n",
       "      <td>Love Wetzel's pretzels! I always get them when...</td>\n",
       "      <td>1</td>\n",
       "      <td>Love Wetzel's pretzels! I always get them when...</td>\n",
       "      <td>[love, wetzel's, pretzels, i, always, get, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Papaya Thai</td>\n",
       "      <td>1</td>\n",
       "      <td>Used a Groupon today to eat here and they gave...</td>\n",
       "      <td>0</td>\n",
       "      <td>Used a Groupon today to eat here and they gave...</td>\n",
       "      <td>[used, a, groupon, today, to, eat, here, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Papaya Thai</td>\n",
       "      <td>3</td>\n",
       "      <td>Ordered the samurai pad tie,vegetable tempura,...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ordered the samurai pad tie,vegetable tempura,...</td>\n",
       "      <td>[ordered, the, samurai, pad, tie, vegetable, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Papaya Thai</td>\n",
       "      <td>1</td>\n",
       "      <td>The owner, \"V\", does not pay employees on time...</td>\n",
       "      <td>0</td>\n",
       "      <td>The owner, \"V\", does not pay_NOT employees_NOT...</td>\n",
       "      <td>[the, owner, v, does, not, pay_not, employees_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Papaya Thai</td>\n",
       "      <td>2</td>\n",
       "      <td>I would give this 3 stars, but I really couldn...</td>\n",
       "      <td>0</td>\n",
       "      <td>I would give this 3 stars, but I_NOT really_NO...</td>\n",
       "      <td>[i, would, give, this, 3, stars, but, i_not, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Torteria Lupita</td>\n",
       "      <td>3</td>\n",
       "      <td>Don't want to be harsh to these compas but I a...</td>\n",
       "      <td>0</td>\n",
       "      <td>Don't want_NOT to_NOT be_NOT harsh_NOT to_NOT ...</td>\n",
       "      <td>[don't, want_not, to_not, be_not, harsh_not, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  stars  \\\n",
       "0   The Empanadas House      5   \n",
       "1      Middle East Deli      3   \n",
       "2      Middle East Deli      3   \n",
       "3          Banzai Sushi      4   \n",
       "4     Wetzel's Pretzels      5   \n",
       "..                  ...    ...   \n",
       "91          Papaya Thai      1   \n",
       "92          Papaya Thai      3   \n",
       "94          Papaya Thai      1   \n",
       "95          Papaya Thai      2   \n",
       "99      Torteria Lupita      3   \n",
       "\n",
       "                                                 text  Sentiment  \\\n",
       "0   I love the empanadas from the Empanadas House!...          1   \n",
       "1   Definitely under new management, and the dinin...          0   \n",
       "2   I will also agree that this place has great fo...          0   \n",
       "3   Been coming here since I was in grade 9 so abo...          1   \n",
       "4   Love Wetzel's pretzels! I always get them when...          1   \n",
       "..                                                ...        ...   \n",
       "91  Used a Groupon today to eat here and they gave...          0   \n",
       "92  Ordered the samurai pad tie,vegetable tempura,...          0   \n",
       "94  The owner, \"V\", does not pay employees on time...          0   \n",
       "95  I would give this 3 stars, but I really couldn...          0   \n",
       "99  Don't want to be harsh to these compas but I a...          0   \n",
       "\n",
       "                                               sample  \\\n",
       "0   I love the empanadas from the Empanadas House!...   \n",
       "1   Definitely under new management, and the dinin...   \n",
       "2   I will also agree that this place has great fo...   \n",
       "3   Been coming here since I was in grade 9 so abo...   \n",
       "4   Love Wetzel's pretzels! I always get them when...   \n",
       "..                                                ...   \n",
       "91  Used a Groupon today to eat here and they gave...   \n",
       "92  Ordered the samurai pad tie,vegetable tempura,...   \n",
       "94  The owner, \"V\", does not pay_NOT employees_NOT...   \n",
       "95  I would give this 3 stars, but I_NOT really_NO...   \n",
       "99  Don't want_NOT to_NOT be_NOT harsh_NOT to_NOT ...   \n",
       "\n",
       "                                               tokens  \n",
       "0   [i, love, the, empanadas, from, the, empanadas...  \n",
       "1   [definitely, under, new, management, and, the,...  \n",
       "2   [i, will, also, agree, that, this, place, has,...  \n",
       "3   [been, coming, here, since, i, was, in, grade,...  \n",
       "4   [love, wetzel's, pretzels, i, always, get, the...  \n",
       "..                                                ...  \n",
       "91  [used, a, groupon, today, to, eat, here, and, ...  \n",
       "92  [ordered, the, samurai, pad, tie, vegetable, t...  \n",
       "94  [the, owner, v, does, not, pay_not, employees_...  \n",
       "95  [i, would, give, this, 3, stars, but, i_not, r...  \n",
       "99  [don't, want_not, to_not, be_not, harsh_not, t...  \n",
       "\n",
       "[88 rows x 6 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    44\n",
       "0    44\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
