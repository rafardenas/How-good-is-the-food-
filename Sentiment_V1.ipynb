{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the 'business' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_json(\"yelp_academic_dataset_business.json\", chunksize = 1000, lines = True)\n",
    "drop_cols = ['address', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open','attributes', 'hours']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deleting non useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "chunks = []\n",
    "a = 0\n",
    "for chunk in df2:\n",
    "    a += 1\n",
    "    chunk_b = chunk.drop(drop_cols, axis = 1)\n",
    "    restas = chunk_b[chunk_b['categories'].str.contains('restaurant', case = False, na = False)]\n",
    "    chunks.append(restas)\n",
    "restaurants = pd.concat(chunks, ignore_index= True, join='outer')\n",
    "end = time.time()\n",
    "elapsed = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63961, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the reviews dataset\n",
    "* Remember we made the merge to use ONLY restaurants data, because there were data from other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_raw = pd.read_json(\"yelp_academic_dataset_review.json\", chunksize=100000, lines = True)\n",
    "drop_cols = ['review_id', 'user_id','useful', 'funny', 'cool', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using merge instead of join because we want to join in another column other than the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for chunk in reviews_raw:\n",
    "    a += 1\n",
    "    reviews = chunk.drop(drop_cols, axis = 1)\n",
    "    data = restaurants.merge(reviews, left_on = 'business_id', right_on = 'business_id',how = 'inner')\n",
    "    if a == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally the data to be preprocessed (the \"text\" column, to be exact) \n",
    "TBD:\n",
    "* Delete all number 3 i.e neutral \n",
    "* Same number of positive as negatives\n",
    "* Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>categories</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lu7vtrp_bE9PnxWfA8g4Pg</td>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>Thornhill</td>\n",
       "      <td>Japanese, Fast Food, Food Court, Restaurants</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Sushi, and unbeatable prices! Only downf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lu7vtrp_bE9PnxWfA8g4Pg</td>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>Thornhill</td>\n",
       "      <td>Japanese, Fast Food, Food Court, Restaurants</td>\n",
       "      <td>3</td>\n",
       "      <td>I don't listen to my father often when it come...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vjTVxnsQEZ34XjYNS-XUpA</td>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Food, Pretzels, Bakeries, Fast Food, Restaurants</td>\n",
       "      <td>4</td>\n",
       "      <td>Never heard of the cheese meltdown pretzel, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vjTVxnsQEZ34XjYNS-XUpA</td>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Food, Pretzels, Bakeries, Fast Food, Restaurants</td>\n",
       "      <td>4</td>\n",
       "      <td>PV Mall's food court needs updating, but that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fnZrZlqW1Z8iWgTVDfv_MA</td>\n",
       "      <td>Carl's Jr</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Mexican, Restaurants, Fast Food</td>\n",
       "      <td>3</td>\n",
       "      <td>I haven't tried much on their menu but their c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id               name       city  \\\n",
       "0  lu7vtrp_bE9PnxWfA8g4Pg       Banzai Sushi  Thornhill   \n",
       "1  lu7vtrp_bE9PnxWfA8g4Pg       Banzai Sushi  Thornhill   \n",
       "2  vjTVxnsQEZ34XjYNS-XUpA  Wetzel's Pretzels    Phoenix   \n",
       "3  vjTVxnsQEZ34XjYNS-XUpA  Wetzel's Pretzels    Phoenix   \n",
       "4  fnZrZlqW1Z8iWgTVDfv_MA          Carl's Jr  Las Vegas   \n",
       "\n",
       "                                         categories  stars  \\\n",
       "0      Japanese, Fast Food, Food Court, Restaurants      5   \n",
       "1      Japanese, Fast Food, Food Court, Restaurants      3   \n",
       "2  Food, Pretzels, Bakeries, Fast Food, Restaurants      4   \n",
       "3  Food, Pretzels, Bakeries, Fast Food, Restaurants      4   \n",
       "4                   Mexican, Restaurants, Fast Food      3   \n",
       "\n",
       "                                                text  \n",
       "0  Great Sushi, and unbeatable prices! Only downf...  \n",
       "1  I don't listen to my father often when it come...  \n",
       "2  Never heard of the cheese meltdown pretzel, bu...  \n",
       "3  PV Mall's food court needs updating, but that ...  \n",
       "4  I haven't tried much on their menu but their c...  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a toyset to work in trials from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66748, 3)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.loc[:, ['name', 'stars', 'text']]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some graphs to know the number of reviews by ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Delete the number 3's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.drop(data[data['stars'] == 3].index, inplace = True)\n",
    "data = data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We want just the reviews with 4-5 to be positive and the 1-2 to be negative, we do that on the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    44050\n",
       "0    13951\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'] = data['stars'].apply(lambda x: 1 if x > 3 else 0)\n",
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use \"3\" as negative to augment the data and  balance the classes? (optional)\n",
    "One could argue that the reviews that have not unconditionally positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_half = list((data[data['stars'] <=3]).index)\n",
    "pos_half = list(set(range(data.shape[0])) - set(neg_half))\n",
    "pos_half = pos_half[0:len(neg_half)]\n",
    "pos_half.extend(neg_half)\n",
    "dropper = list(set(range(data.shape[0])) - set(pos_half))\n",
    "data.drop(index = dropper, inplace = True)\n",
    "data['Sentiment'] = data['stars'].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    13951\n",
       "0    13951\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here start the NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to make a bag of words, it can be done manually, but also with sklearn.\n",
    "\n",
    "**Steps:** I am trying to extract the nest tokens with the tokenizer from Potts and then feed that already \"clean tokens\" to the vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC for NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = data2['text'][1:3]\n",
    "raw\n",
    "raw2 = data2['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "counter = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = []\n",
    "a = 0\n",
    "for i in raw2:\n",
    "    words = i.lower()\n",
    "    words = words.split()\n",
    "    for word in words:\n",
    "        a += 1\n",
    "        counter[word] += 1\n",
    "        if word not in tokenizer:\n",
    "            tokenizer.append(word)\n",
    "len(tokenizer)\n",
    "tokenizer\n",
    "words = raw2.split()\n",
    "for word in words:\n",
    "    a += 1\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the negation tagging, put the negation until ^[.:;!?]$ (until the punctuation mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#out of the box tokenizer and counter\n",
    "from nltk.probability import FreqDist\n",
    "counter = FreqDist()\n",
    "tokens_nltk = word_tokenize(raw2)\n",
    "\n",
    "tokenizer = []\n",
    "for word in tokens_nltk:\n",
    "    counter[word.lower()] += 1\n",
    "\n",
    "len(tokens_nltk)\n",
    "len(counter)\n",
    "counter['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter.B() #is the number of unique words (?)\n",
    "#x = counter.N #is the number of words\n",
    "#counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials for regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD:\n",
    "### Check the paper that is mentioned in notion by UCLondon\n",
    "\n",
    "* Delete all the reviews that are not in english\n",
    "* Same number of negative that as positive reviews for the sets\n",
    "* Tokenization and BoW creation (**BoW with frequency or with presence?**)\n",
    "* unigrams and bigrams\n",
    "* lower the case\n",
    "* POS tagging?\n",
    "* EDA como en potts con las palabras mas frecuentes en positives and negatives reviews\n",
    "* Handling negation? _NOT or with sentiment negative scoring?\n",
    "* See the book by Bing Lui for how to identify fake news, resonates with anomaly detection and identifying if a review is fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123456789', '987654321', '987654321']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Been coming here since I was in grade 9 so about 10 years now (wow!) staff are very friendly and prices are ridiculously cheap. I remember back in my younger days being short on change and they never cared! Super nice family owned businesses. I always get the California roll, either I grab one out of the fridge or have them make it fresh if there's none. The tofu is also really good! They also sell so many different kinds of pop in a can for a buck and different Asian treats like pocky!\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\"Hello my Number is 123456789 and  \n",
    "             my friend's number is 987654321, and my number is also 987654321\"\"\"\n",
    "    \n",
    "# A sample regular expression to find digits.  \n",
    "regex = '\\d+'             \n",
    "    \n",
    "match = re.findall(regex, string)  \n",
    "print(match)\n",
    "raw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation tagging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesimist(text):\n",
    "    x = text\n",
    "    x = x.split()\n",
    "    hasta = 0\n",
    "    desde = 0\n",
    "    c = 0\n",
    "    j = 0\n",
    "    passer = False\n",
    "    for k in range(len(x)):\n",
    "        #print(j)\n",
    "        #print(k)\n",
    "        #if not passer:\n",
    "            #continue\n",
    "        passer = True\n",
    "\n",
    "        i = x[k]\n",
    "        negation_string = r\"\"\"\n",
    "        ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "            havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "            wont|wouldnt|but|doesnt|didnt|isnt|arent|aint\n",
    "        )$|n't\n",
    "        \"\"\"\n",
    "        #print(\"first\")\n",
    "        c +=1\n",
    "        #match = re.search(r'\\bthe\\b',i)\n",
    "        neg = re.compile(negation_string, re.VERBOSE | re.I | re.UNICODE)\n",
    "        match = neg.findall(i)\n",
    "        #print(i + \"i\")\n",
    "        #print('desde', desde)\n",
    "        #print(f'hasta menos desde es: {hasta - desde} y c es {c}')\n",
    "        if c < (hasta - desde):\n",
    "            continue\n",
    "        if match:\n",
    "            c = 0\n",
    "            desde = k + 1\n",
    "            #print(c - 1)\n",
    "            #j = i\n",
    "            jump = k\n",
    "            for j in range(jump, 100):\n",
    "\n",
    "                #print(j)\n",
    "                try:\n",
    "                    comma = re.search(r'[.:;!?]', x[j])\n",
    "                    #print(\"second\")\n",
    "                    if comma:\n",
    "                        hasta = j + 1\n",
    "                        c += 1\n",
    "                        \n",
    "                        #this try to avoid error if there is no punctuation error before the phrase ends\n",
    "                        try:\n",
    "                            for i in range(desde, hasta):\n",
    "                                repl = re.match(r'\\w+', x[i])\n",
    "                                x[i] = repl.group() + \"_NOT\"\n",
    "                            \n",
    "                            c = 0\n",
    "                            break\n",
    "                        except:\n",
    "                            #print(' '.join(x))\n",
    "                            c = 0\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "        if match:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "                #continue\n",
    "\n",
    "\n",
    "            #else:\n",
    "                #continue\n",
    "            #break\n",
    "    xx = ' '.join(x)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Already done*\n",
    "    1. Identify all the negation words on the regular expresion, can be taken from one paper.\n",
    "    2. Implement it i conjuction with the tokenizer and the stop words removal\n",
    "    3. run it for all the dataset\n",
    "    4. Balance the positive and negative classes on the data set that we are going to take to make all the trials.\n",
    "    5. Finish the identification of features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current status\n",
    "\n",
    "order of the pipeline (in **bold** what is already done)\n",
    "\n",
    "(balance the sample 50/50 in reviews)\n",
    "1. **Negator**\n",
    "2. **Tokenizer**\n",
    "3. **Stop words removal (kind of done, have to figure it out)**\n",
    "3. **BoW**\n",
    "\n",
    "*Follows: Select the features and extend the functionality for all the reviews*\n",
    "1. bigram\n",
    "2. Positive Tokens\n",
    "3. Negative Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trial = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function of negation taggin to each row\n",
    "trial[\"sample\"] = trial.loc[:, \"text\"].apply(pesimist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Sushi, and unbeatable prices! Only downf...</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Sushi, and unbeatable prices! Only downf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>4</td>\n",
       "      <td>Never heard of the cheese meltdown pretzel, bu...</td>\n",
       "      <td>1</td>\n",
       "      <td>Never heard_NOT of_NOT the_NOT cheese_NOT melt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>4</td>\n",
       "      <td>PV Mall's food court needs updating, but that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>PV Mall's food court needs updating, but that_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carl's Jr</td>\n",
       "      <td>4</td>\n",
       "      <td>I'm a Carl's Jr fan for their fried zucchini a...</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm a Carl's Jr fan for their fried zucchini a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carl's Jr</td>\n",
       "      <td>4</td>\n",
       "      <td>Customer service has been top notch on every v...</td>\n",
       "      <td>1</td>\n",
       "      <td>Customer service has been top notch on every v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  stars  \\\n",
       "0       Banzai Sushi      5   \n",
       "1  Wetzel's Pretzels      4   \n",
       "2  Wetzel's Pretzels      4   \n",
       "3          Carl's Jr      4   \n",
       "4          Carl's Jr      4   \n",
       "\n",
       "                                                text  Sentiment  \\\n",
       "0  Great Sushi, and unbeatable prices! Only downf...          1   \n",
       "1  Never heard of the cheese meltdown pretzel, bu...          1   \n",
       "2  PV Mall's food court needs updating, but that ...          1   \n",
       "3  I'm a Carl's Jr fan for their fried zucchini a...          1   \n",
       "4  Customer service has been top notch on every v...          1   \n",
       "\n",
       "                                              sample  \n",
       "0  Great Sushi, and unbeatable prices! Only downf...  \n",
       "1  Never heard_NOT of_NOT the_NOT cheese_NOT melt...  \n",
       "2  PV Mall's food court needs updating, but that_...  \n",
       "3  I'm a Carl's Jr fan for their fried zucchini a...  \n",
       "4  Customer service has been top notch on every v...  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.happyfuntokenizing import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preserve_case': False, 'all_in': False}"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tok.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial[\"tokens\"] = trial.loc[:, \"sample\"].apply(tok.tokenize)\n",
    "trial.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    13951\n",
       "0    13951\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing 'filler' words\n",
    "Counting the most common words over all the corpora we can tell that the most common ones, and thus, tne ones that may play a bid role on the classification, are not meaningful for us to discover which words are really expressing a positive or negative emotion, thus, the first 15 words are going to be removed from the corpus.\n",
    "\n",
    "In the following cells, the most common words are shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = trial['tokens'].to_numpy()\n",
    "review1 = reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'sushi',\n",
       " 'and',\n",
       " 'unbeatable',\n",
       " 'prices',\n",
       " 'only',\n",
       " 'downfall',\n",
       " 'is',\n",
       " 'that',\n",
       " 'they',\n",
       " 'are',\n",
       " 'cash',\n",
       " 'only',\n",
       " 'and',\n",
       " 'close',\n",
       " 'by',\n",
       " '7pm']"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_words(raw_reviews, w_number):\n",
    "    \"\"\"\n",
    "    Creates a list with the most 'w_number' (number) of words on the whole corpus\n",
    "    ---------\n",
    "    raw_reviews : all the column of the raw reviews\n",
    "    w_number = number of most common words that wish to be extracted\n",
    "    \"\"\"\n",
    "    \n",
    "    commons = []\n",
    "    for word in counting:\n",
    "        commons.append(word[0])\n",
    "        if len(commons) == w_number:\n",
    "            break\n",
    "    return commons, counter\n",
    "cc, count = most_common_words(reviews, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 144056), ('and', 97062), ('i', 70720), ('a', 65661), ('to', 61725), ('was', 57025), ('of', 39427), ('it', 35423), ('for', 32693), ('is', 31724), ('we', 29998), ('in', 27953), ('my', 23342), ('this', 23075), ('with', 22284), ('that', 22265), ('but', 21756), ('they', 20238), ('food', 19773), ('not', 19668), ('on', 18536), ('were', 17890), ('had', 17876), ('you', 16784), ('have', 15084), ('so', 14639), ('the_not', 13974), ('at', 13970), ('place', 13797), ('good', 13268), ('our', 11915), ('are', 11839), ('be', 11175), ('service', 10710), ('as', 10688), ('there', 10300), ('very', 10113), ('out', 9576), ('great', 9481), ('just', 9248), ('if', 9199), ('here', 9143), ('like', 8918), ('all', 8813), ('to_not', 8510), ('me', 8475), ('time', 8220), ('one', 8210), ('when', 7755), ('back', 7569), ('their', 7394), ('would', 7348), ('ordered', 7247), ('get', 7073), ('no', 7064), ('from', 7054), ('a_not', 6823), ('up', 6804), ('which', 6649), ('...', 6630), ('and_not', 6627), ('us', 6584), ('will', 6410), ('an', 6360), ('or', 6314), ('order', 6309), ('go', 6253), ('it_not', 6132), (\"it's\", 6130), ('she', 6109), ('really', 6105), ('restaurant', 6038), ('about', 5893), ('i_not', 5881), ('he', 5589), ('got', 5588), ('only', 5575), ('what', 5529), ('chicken', 5459), ('been', 5444), ('also', 5346), ('came', 5320), ('some', 5319), ('by', 4990), (\"don't\", 4948), ('more', 4934), ('because', 4855), ('after', 4724), ('was_not', 4684), ('can', 4644), ('your', 4595), ('even', 4515), ('never', 4422), ('other', 4387), (\"didn't\", 4341), ('menu', 4256), ('too', 4249), ('them', 4120), ('went', 4110), ('first', 4105), (\"i'm\", 4046), ('nice', 4003), ('well', 3992), ('best', 3906), ('then', 3778), ('over', 3776), (\"i've\", 3774), ('of_not', 3769), ('try', 3761), ('delicious', 3729), ('table', 3717), ('minutes', 3629), ('has', 3601), ('could', 3569), ('did', 3532), ('said', 3529), ('come', 3513), ('do', 3507), ('love', 3487), ('staff', 3465), ('for_not', 3422), ('again', 3378), ('than', 3375), ('always', 3293), ('better', 3286), ('people', 3261), ('definitely', 3241), ('little', 3218), ('asked', 3218), ('sauce', 3216), ('pizza', 3166), ('that_not', 3150), ('in_not', 3127), ('two', 3119), ('eat', 3101), ('experience', 3075), ('her', 3071), ('salad', 3049), ('made', 3002), ('off', 2990), ('much', 2950), ('night', 2946), ('bar', 2924), ('pretty', 2922), ('amazing', 2885), ('cheese', 2866), ('friendly', 2863), ('dinner', 2859), ('2', 2849), ('server', 2830), ('meal', 2829), ('way', 2813), ('ever', 2790), ('how', 2778), ('fresh', 2766), ('lunch', 2741), ('make', 2740), ('but_not', 2729), ('took', 2728), ('wait', 2719), ('is_not', 2694), ('who', 2681), ('before', 2632), ('going', 2616), ('this_not', 2608), ('down', 2553), ('told', 2552), ('sushi', 2514), ('bad', 2505), ('say', 2505), ('give', 2483), (\"wasn't\", 2472), ('everything', 2451), ('times', 2449), ('another', 2438), ('rice', 2422), ('while', 2415), ('drinks', 2410), ('food_not', 2381), ('still', 2364), ('right', 2362), ('know', 2342), ('take', 2325), ('around', 2314), ('want', 2275), ('fries', 2250), ('be_not', 2244), ('few', 2243), ('think', 2237), ('have_not', 2235), ('am', 2221), ('tried', 2207), ('small', 2201), ('now', 2181), ('they_not', 2173), ('at_not', 2169), ('side', 2147), ('new', 2135), ('5', 2131), ('location', 2122), ('on_not', 2116), ('..', 2113), ('any', 2112), ('we_not', 2112), ('last', 2108), ('meat', 2105), ('since', 2105), ('hot', 2083), ('next', 2077), ('you_not', 2054), ('not_not', 2051), ('something', 2042), ('waitress', 2034), ('both', 2031), ('day', 2008), ('being', 1986), ('should', 1979), ('my_not', 1976), (\"can't\", 1958), ('3', 1951), ('manager', 1950), ('price', 1950), ('dish', 1942), ('soup', 1939), ('though', 1928), ('sure', 1918), ('many', 1913), ('every', 1904), ('drink', 1904), ('recommend', 1903), ('quality', 1896), ('area', 1895), ('most', 1883), ('with_not', 1877), ('see', 1867), ('burger', 1865), ('wanted', 1863), ('thing', 1860), ('bit', 1852), ('long', 1850), ('his', 1841), ('nothing', 1838), ('so_not', 1827), ('good_not', 1826), ('fried', 1819), ('sandwich', 1819), ('ok', 1819), ('bread', 1817), ('beef', 1806), ('tasted', 1798), ('steak', 1790), ('taste', 1784), ('vegas', 1775), ('where', 1719), ('left', 1716), ('hour', 1706), ('excellent', 1691), ('customer', 1658), ('dishes', 1657), ('same', 1652), ('....', 1650), ('10', 1644), ('lot', 1630), ('ask', 1607), ('however', 1605), ('4', 1591), ('stars', 1591), ('had_not', 1581), ('atmosphere', 1581), ('away', 1567), ('back_not', 1565), ('fish', 1565), ('pork', 1564), ('sweet', 1548), ('cold', 1546), ('favorite', 1544), ('awesome', 1540), ('different', 1535), ('beer', 1534), ('super', 1521), ('places', 1510), ('all_not', 1508), ('place_not', 1507), ('as_not', 1507), ('into', 1505), ('flavor', 1504), ('like_not', 1499), ('shrimp', 1499), ('friends', 1489), ('cooked', 1486), ('home', 1482), ('maybe', 1476), ('served', 1474), ('looking', 1474), ('waiter', 1473), ('half', 1470), ('happy', 1469), ('check', 1466), ('big', 1456), ('looked', 1452), ('tasty', 1451), ('friend', 1451), ('probably', 1449), ('find', 1448), ('its', 1446), ('spicy', 1444), ('or_not', 1442), ('even_not', 1433), ('breakfast', 1430), ('special', 1424), ('here_not', 1417), ('prices', 1413), ('top', 1413), ('finally', 1404), ('1', 1398), ('why', 1398), (\"that's\", 1391), ('overall', 1389), ('tables', 1388), ('husband', 1385), ('actually', 1383), ('visit', 1379), ('decided', 1378), ('enough', 1375), ('cream', 1370), ('room', 1363), ('waited', 1360), ('coffee', 1360), ('there_not', 1355), ('eating', 1353), ('go_not', 1347), ('worth', 1345), ('disappointed', 1341), ('get_not', 1330), ('tacos', 1326), ('worst', 1324), ('coming', 1313), ('gave', 1301), ('once', 1292), ('water', 1289), ('things', 1286), ('each', 1285), ('were_not', 1281), ('family', 1273), ('full', 1269), ('waiting', 1268), ('thought', 1264), ('least', 1263), ('quite', 1257), (\"won't\", 1252), ('large', 1251), ('restaurants', 1239), ('one_not', 1236), ('old', 1226), ('used', 1215), ('ice', 1215), (\"you're\", 1212), ('perfect', 1209), ('roll', 1205), ('busy', 1203), ('hard', 1201), ('rolls', 1197), ('just_not', 1191), ('20', 1188), ('sat', 1185), ('else', 1183), ('work', 1178), ('dessert', 1169), ('feel', 1166), ('these', 1162), ('found', 1156), ('me_not', 1155), ('need', 1152), ('plate', 1152), ('wine', 1151), ('put', 1146), ('without', 1145), ('service_not', 1144), ('items', 1143), ('loved', 1142), ('almost', 1140), ('him', 1139), ('spot', 1130), ('brought', 1129), ('review', 1127), (\"i'll\", 1126), ('star', 1125), ('decent', 1125), ('walked', 1123), ('later', 1122), ('seated', 1122), ('called', 1113), ('horrible', 1112), ('clean', 1103), (\"couldn't\", 1098), ('money', 1097), ('thai', 1089), ('town', 1085), ('inside', 1084), ('kind', 1080), ('terrible', 1079), ('wings', 1078), ('must', 1075), ('huge', 1073), ('what_not', 1072), ('couple', 1070), ('dining', 1066), ('open', 1064), ('getting', 1060), ('arrived', 1056), ('usually', 1054), ('everyone', 1054), ('fast', 1053), ('free', 1053), ('15', 1052), ('about_not', 1050), (\"i'd\", 1050), ('very_not', 1048), ('seemed', 1037), ('point', 1037), ('dry', 1036), ('chips', 1031), ('enjoyed', 1029), ('second', 1027), ('high', 1026), ('far', 1022), ('such', 1020), ('three', 1015), ('bill', 1012), ('wrong', 1011), ('reviews', 1010), ('years', 1009), ('absolutely', 1008), ('time_not', 1005), ('customers', 993), ('extra', 992), ('party', 991), ('ate', 989), ('owner', 989), ('anything', 989), ('house', 988), ('our_not', 988), ('tell', 987), ('outside', 981), ('instead', 977), ('business', 977), ('are_not', 971), ('out_not', 971), ('order_not', 969), ('options', 966), ('line', 965), ('wife', 964), ('front', 963), ('done', 963), ('rude', 959), ('look', 958), ('been_not', 958), ('those', 952), ('slow', 951), ('pay', 949), ('okay', 942), ('let', 938), ('through', 937), ('enjoy', 937), ('someone', 936), ('having', 931), ('selection', 930), ('group', 930), ('kitchen', 928), ('comes', 928), ('trying', 926), ('today', 921), ('bowl', 919), ('person', 919), ('us_not', 917), ('tea', 915), ('fun', 911), ('when_not', 906), ('highly', 905), ('quick', 904), ('portions', 904), ('if_not', 902), ('bland', 899), ('bacon', 899), ('especially', 894), ('end', 892), ('oh', 890), ('really_not', 888), ('started', 888), ('during', 886), ('orders', 884), ('extremely', 879), ('brunch', 876), ('felt', 873), ('again_not', 872), ('from_not', 871), ('door', 870), ('whole', 870), ('bbq', 867), ('part', 862), ('several', 858), ('fantastic', 856), ('french', 854), ('their_not', 852), ('servers', 852), ('week', 850), ('great_not', 848), (\"isn't\", 845), ('much_not', 845), ('salmon', 843), ('bring', 839), ('close', 838), ('egg', 838), ('up_not', 837), ('red', 834), ('appetizer', 832), ('ordering', 830), ('green', 821), ('any_not', 819), (\"doesn't\", 817), ('course', 812), ('attentive', 811), ('hours', 805), ('chinese', 804), ('noodles', 804), (\"wouldn't\", 803), ('reason', 802), ('wish', 795), ('30', 794), ('fine', 794), ('too_not', 794), ('return', 790), ('less', 789), ('saw', 789), ('guy', 787), ('leave', 785), ('may', 783), ('start', 782), ('6', 775), ('pho', 771), ('guess', 771), ('chef', 770), ('bad_not', 769), ('call', 767), ('music', 764), ('live', 763), ('plus', 762), ('poor', 760), ('salsa', 760), ('portion', 760), ('average', 752), ('eggs', 751), ('name', 750), ('because_not', 750), ('yelp', 747), ('cool', 746), ('until', 746), ('yes', 745), ('empty', 745), ('wait_not', 744), ('buffet', 744), ('decor', 742), ('does', 741), ('wonderful', 735), ('pasta', 735), ('keep', 734), ('late', 726), ('mexican', 725), ('size', 725), ('stop', 723), (\"there's\", 723), ('hotel', 723), ('sit', 721), ('know_not', 721), ('kids', 719), ('although', 719), ('eat_not', 718), ('no_not', 718), ('might', 718), ('use', 717), ('perfectly', 717), ('sitting', 709), ('drive', 700), ('crispy', 698), ('walk', 698), ('own', 698), ('more_not', 698), ('tip', 697), ('given', 697), ('warm', 696), ('serve', 695), ('grilled', 695), ('already', 692), ('ended', 686), ('either', 685), ('short', 684), ('pieces', 683), ('come_not', 683), ('white', 680), ('crab', 678), ('making', 678), ('paid', 675), ('an_not', 673), ('completely', 672), ('makes', 672), ('street', 668), ('literally', 665), ('would_not', 662), ('liked', 661), ('taco', 661), ('burrito', 658), ('worth_not', 654), ('amount', 652), ('style', 652), ('only_not', 651), ('your_not', 650), ('regular', 649), ('twice', 648), ('hostess', 647), ('real', 646), ('ago', 644), ('bartender', 642), ('yet', 639), ('seems', 637), ('cheap', 637), ('garlic', 636), ('stay', 636), ('patio', 636), ('offer', 635), ('curry', 634), ('care', 634), ('restaurant_not', 633), ('lots', 632), ('delivery', 631), ('chocolate', 626), ('myself', 625), ('potatoes', 623), ('birthday', 623), ('fact', 620), ('entire', 620), ('eaten', 619), ('soon', 617), ('employees', 616), ('plates', 616), ('bite', 614), ('potato', 613), ('stopped', 612), ('expensive', 612), ('them_not', 611), ('evening', 611), ('seating', 609), ('quickly', 609), ('can_not', 609), ('expect', 608), ('received', 607), ('counter', 605), ('awful', 604), ('boyfriend', 601), ('disappointing', 600), ('cut', 598), ('rather', 597), ('main', 596), ('strip', 596), ('parking', 595), ('year', 594), ('butter', 594), ('sorry', 593), ('burgers', 592), ('other_not', 592), ('four', 592), ('yummy', 590), ('saturday', 590), ('appetizers', 590), ('meals', 589), ('glass', 589), ('pick', 586), ('authentic', 585), ('needed', 583), ('anyone', 579), ('deal', 579), ('priced', 578), ('salty', 578), ('store', 577), ('ready', 575), ('sunday', 575), ('under', 574), ('sandwiches', 573), ('past', 568), ('cake', 566), ('working', 566), ('charge', 565), ('beans', 564), ('add', 563), ('seafood', 563), ('lady', 562), ('unfortunately', 561), ('offered', 559), ('looks', 558), (\"they're\", 557), ('rest', 557), (\"weren't\", 556), ('menu_not', 554), ('tuna', 553), ('reservation', 553), ('card', 550), ('medium', 549), ('mediocre', 549), ('tender', 548), ('phone', 546), ('ingredients', 546), ('local', 544), ('going_not', 542), ('pm', 542), ('light', 541), ('choice', 540), ('toast', 540), ('management', 538), ('show', 537), ('ambiance', 536), ('black', 534), ('dirty', 533), ('do_not', 533), ('between', 531), ('vegan', 530), ('days', 528), ('variety', 528), ('overpriced', 527), ('how_not', 526), ('seriously', 525), ('hungry', 524), ('ribs', 524), ('8', 523), ('cook', 521), ('didn_not', 521), ('often', 519), ('behind', 519), ('reasonable', 518), ('totally', 518), ('gone', 517), ('job', 517), ('please', 516), ('piece', 516), ('italian', 515), ('will_not', 513), ('girl', 513), ('las', 513), ('able', 512), ('de', 511), ('sure_not', 509), ('taste_not', 508), ('list', 507), ('kept', 506), ('simple', 506), ('by_not', 506), ('anything_not', 505), ('recommended', 501), ('korean', 500), ('onion', 499), ('flavors', 499), ('taking', 499), ('opened', 498), ('heard', 498), ('early', 498), ('la', 493), ('guys', 493), ('stuff', 492), ('after_not', 489), ('prepared', 487), ('wow', 487), ('believe', 486), ('lettuce', 486), ('sides', 486), ('problem', 486), ('honestly', 485), ('crust', 484), ('hope', 484), ('saying', 483), ('five', 483), ('excited', 482), ('together', 480), ('lobster', 480), ('7', 478), ('fan', 477), ('tonight', 476), ('etc', 476), (':)', 474), ('corn', 474), ('helpful', 474), ('near', 474), ('specials', 474), ('others', 473), ('trip', 472), ('flavorful', 471), ('want_not', 471), ('help', 470), ('make_not', 467), ('change', 465), ('entrees', 465), ('soft', 464), ('doing', 463), ('combo', 463), ('friday', 463), ('expected', 463), ('gets', 462), ('serving', 461), ('entree', 460), (\"haven't\", 460), ('mean', 459), ('game', 458), ('sour', 457), ('than_not', 457), ('immediately', 456), ('says', 456), ('needs', 455), ('understand', 454), ('sausage', 454), ('tomato', 451), ('choices', 450), ('salads', 450), ('got_not', 450), ('dog', 449), ('plenty', 449), ('attitude', 449), ('lamb', 448), ('dressing', 447), ('man', 446), ('mind', 445), ('thank', 443), ('across', 443), ('sad', 442), ('walking', 441), ('loud', 440), ('seem', 438), ('try_not', 438), ('min', 436), ('better_not', 436), ('she_not', 436), ('yourself', 435), ('run', 435), ('mac', 434), ('based', 434), ('noticed', 433), ('surprised', 433), ('veggies', 433), ('greasy', 433), ('floor', 433), ('life', 432), ('total', 429), ('tiny', 429), ('asian', 428), ('charged', 428), ('basically', 428), ('giving', 427), ('hit', 427), ('some_not', 427), ('barely', 426), ('mouth', 426), ('beautiful', 426), (\"we've\", 426), ('available', 425), ('vegetarian', 424), ('itself', 423), ('beers', 423), ('did_not', 422), ('burnt', 421), ('due', 421), ('set', 421), ('added', 420), ('salt', 418), ('hands', 417), ('mins', 416), ('busy_not', 416), ('over_not', 415), ('tastes', 413), ('weekend', 413), ('choose', 412), ('ever_not', 412), ('never_not', 412), ('morning', 412), ('say_not', 410), ('attention', 409), ('cup', 409), ('filled', 409), ('rib', 409), ('interesting', 408), ('onions', 407), ('option', 407), ('disgusting', 407), ('knew', 407), ('date', 407), ('came_not', 406), ('think_not', 406), ('type', 406), ('still_not', 406), ('including', 405), ('japanese', 405), ('establishment', 404), ('closed', 404), ('minute', 403), ('downtown', 402), ('sauces', 401), ('has_not', 401), ('experience_not', 401), ('glad', 400), ('table_not', 400), ('n', 400), ('give_not', 399), ('before_not', 399), ('way_not', 398), ('along', 397), ('easy', 396), ('chicken_not', 396), ('best_not', 396), ('remember', 395), ('packed', 395), ('impressed', 394), ('forgot', 394), ('seat', 394), ('don_not', 393), ('asking', 392), ('rare', 391), ('enough_not', 390), ('which_not', 390), ('baked', 390), ('oil', 389), ('finish', 388), ('disappointed_not', 388), ('noodle', 388), ('flavor_not', 388), ('paying', 388), ('toppings', 387), ('take_not', 387), ('pleasant', 387), ('tasting', 386), ('thin', 386), ('finished', 384), ('chili', 384), (\"we're\", 384), ('cafe', 384), ('greeted', 383), ('talk', 382), ('simply', 382), ('hand', 382), ('people_not', 381), ('frozen', 380), ('45', 380), ('upon', 380), ('dim', 380), ('phoenix', 379), ('.....', 379), ('fresh_not', 379), ('spring', 377), ('sum', 377), ('cute', 376), ('box', 376), ('worse', 376), ('watch', 375), ('placed', 375), ('cashier', 374), ('seen', 373), ('chance', 373), ('25', 373), ('pepper', 373), ('grill', 373), ('bouchon', 372), ('12', 371), ('number', 371), ('soggy', 370), ('pie', 370), ('ones', 369), ('space', 369), ('pricey', 369), ('forward', 368), ('bean', 367), ('disappointment', 366), ('special_not', 366), ('ordered_not', 365), ('shop', 365), ('spinach', 365), ('turned', 364), ('low', 363), ('read', 363), ('per', 363), ('thanks', 362), ('taken', 362), ('avoid', 361), ('unique', 361), ('sign', 360), ('clearly', 359), ('note', 358), ('visited', 358), ('pad', 358), ('somewhere', 358), ('vegetables', 357), ('he_not', 356), ('sometimes', 356), ('single', 356), ('mix', 354), ('item', 354), ('veggie', 354)]\n"
     ]
    }
   ],
   "source": [
    "a= count.most_common()\n",
    "print(a[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restricted_corpus_builder(corpus_size, raw_reviews):\n",
    "    \"\"\"\n",
    "    Create the bag of words with the first 'corpus_size' most common words present on the reviews, \n",
    "    n\n",
    "    -----------\n",
    "    corpus_size : number of words to be included on de bow\n",
    "    raw_reviews : all the column of the raw reviews\n",
    "    \"\"\"\n",
    "    bow = []\n",
    "    bow_counter = []\n",
    "    counter = FreqDist()\n",
    "    for review in raw_reviews:\n",
    "        for word in review:\n",
    "            counter[word] += 1\n",
    "    counting = counter.most_common(corpus_size)\n",
    "    for word in counting:\n",
    "        bow.append(word[0])\n",
    "        bow_counter.append(word[1])\n",
    "    return bow, bow_counter  \n",
    "bow, bow_counted = restricted_corpus_builder(1500, reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "Using the tokens, of course\n",
    "* Make the corpora\n",
    "* Make the vectors with word presence/frecuency. I think presenc may be better for vector-space representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD: check why is not working when the number of ommited words is omre than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_builder(raw_reviews, most_common):\n",
    "    \"\"\"\n",
    "    Create the bag of words of all the words present on the reviews, ommiting the 'most_common' words as they are \n",
    "    conseidered as fillers with low influence on the classification\n",
    "    -----------\n",
    "    raw_reviews : all the column of the raw reviews\n",
    "    most_common : list of most common words that wish to be ommited\n",
    "    \"\"\"\n",
    "    main_corpus = []\n",
    "    for review in raw_reviews:\n",
    "        for word in review:\n",
    "            if word in most_common:\n",
    "                continue\n",
    "            if word not in main_corpus:\n",
    "                main_corpus.append(word)\n",
    "    return main_corpus\n",
    "#bow = corpus_builder(reviews, cc)\n",
    "#len(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(bow, ind_review, ommitted_words):\n",
    "    \"\"\"\n",
    "    Vectorize the review based on the counting of the words present on the review, \n",
    "    the appereances are counted on a dictionary, the value of the words (keys) that are not present remain as '0'\n",
    "    ----------\n",
    "    bow : bag of all the words in list (unique values)\n",
    "    vector: individual review tokenized on the form of a list\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    counter = dict.fromkeys(bow, 0)\n",
    "    for word in ind_review:\n",
    "        try:\n",
    "            index += 1\n",
    "            if word in ommitted_words:\n",
    "                continue\n",
    "            else:\n",
    "                counter[word] += 1\n",
    "        except:\n",
    "            pass\n",
    "    return list(counter.values()), index\n",
    "#vv = vectorizer(bow, review1, cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_builder(bow, review_series, ommited_words):\n",
    "    \"\"\"\n",
    "    Creates the matrix of features based on the term frecuency vectors created by the function vectorizer\n",
    "    ----------\n",
    "    review_series : pandas object (series)  i.e dataframe[name_of_column]\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    i = 0\n",
    "    \n",
    "    for review in review_series:\n",
    "        word_vector, index = vectorizer(bow, review, ommited_words)\n",
    "        if i < 1:\n",
    "            X = np.array([word_vector])\n",
    "            i += 1\n",
    "            continue\n",
    "        X = np.append(X, [word_vector], axis = 0)\n",
    "            \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for the creation of the feature matrix\n",
    "* features thus far: \n",
    "    * unigram i.e. word frecuency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'is'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-5d19fb4e64da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost_common_in_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-202-2ae7e60ce572>\u001b[0m in \u001b[0;36mmatrix_builder\u001b[0;34m(bow, review_series, ommited_words)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview_series\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mword_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mommited_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-198-90f7face423d>\u001b[0m in \u001b[0;36mvectorizer\u001b[0;34m(bow, ind_review, ommitted_words)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mvv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'is'"
     ]
    }
   ],
   "source": [
    "reviews = trial['tokens'].to_numpy()\n",
    "common = most_common_words(reviews, 9)\n",
    "bow = corpus_builder(reviews, common)\n",
    "raw_reviews = trial['tokens']\n",
    "X = matrix_builder(bow, raw_reviews, common)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_reviews = trial['tokens']\n",
    "raw_reviews = raw_reviews[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1500)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common = []\n",
    "X = matrix_builder(bow, raw_reviews, common)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, ..., 0, 0, 0],\n",
       "       [2, 1, 0, ..., 0, 0, 0],\n",
       "       [5, 2, 3, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [8, 3, 5, ..., 0, 0, 0],\n",
       "       [2, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_vector = trial['Sentiment'].to_numpy()\n",
    "target = np.asarray([target_vector[0:1000]])\n",
    "target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.reshape(target, ([target.shape[1],-1]))\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-d76cbfc21766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
