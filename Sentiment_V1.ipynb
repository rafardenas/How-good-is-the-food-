{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How good is the food?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rafael Cardenas* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: This project is still under construction, at this date (19.07.20) the classification algorithm is being fine tunned to increase its performance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Outline\n",
    "\n",
    "\n",
    "1. [Problem background and business value](#Problem_Backgound)\n",
    "2. [Objectives and problem statement](#Objectives)\n",
    "3. [EDA](#EDA)\n",
    "4. Methodology\n",
    "5. Algorithm\n",
    "6. Conclusions and next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "<a id='Problem_Backgound'><a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis has raised in parallel with the ammount of opinions on the web. Part of a broader field known as opinion mining, in the early 2000's gained popularity as businesses started to receive more and more feedback for their products in the form of comments.\n",
    "As data volumes began to increase in size, methods to extract information were developed, sentiment analysis is still considered as a robust one to quantify opinions of the masses.\n",
    "\n",
    "Fast forward to more recent times, sentiment analysis has extended farther than product reviews; now ranging from *brand perception* to opinion mining in sensible topics such as the elections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Business Value:\n",
    "At the end of the day, data **science in a company serves the ultimate goal of a business: Profit generation.** Sentiment Analysis has a broad range of applications in the real world. Successful companies mine opinions of a specific brand/product from several sources to quickly know what is popular opinion of the product/brand and identify the possible reasons; with that, companies could make better decisions in difficult times.\n",
    "In this same vein, businesses can identify which are the most important factors that lead the costumer to incline towards a positive or negative opinion and based on that, make the pertinent moves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives and problem statement\n",
    "<a id='Objectives'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has been done from a didactical point of view, approaching the problem with the combination of already existing resources and my own initiative. That being said, I did set specific goals and success metrics for this project:\n",
    "\n",
    "* No libraries *(NLTK, SpaCy)* NLP pipeline: Self imposed restriction to manually implement text preparation and feature extraction.\n",
    "* At least 85% accuracy on classification, taking as benchmark papers on Sentiment Analysis **([Pang, et al. 2002](https://www.aclweb.org/anthology/W02-1011/) & [Potts, et al. 2011](https://web.stanford.edu/~cgpotts/papers/potts-salt20-negation.pdf))**\n",
    "* Deploy the model. ML models that does not go beyond a notebook provide no value to the business, for a model to be useful, an API should be available\n",
    "\n",
    "##### Problem Statement\n",
    "\n",
    "The questions that are going to be answer through this analysis.\n",
    "\n",
    "1. What makes a good/bad review?\n",
    "2. In which aspects should restaurants improbe to gain/mantain customers?\n",
    "3. Can we identify fake reviews? i.e reviews written by owners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "<a id='EDA'><a/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep the focus of this project on the analysis part of the pipeline, I used the [*Yelp Dataset*](https://www.yelp.com/dataset/download) provided by Yelp.\n",
    "This part of the documentation focuses on the initial analysis of the data to make myself familiar with the data and make my first assumptions and paths that I could take towards the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting with the preprocessing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the 'business' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_json(\"yelp_academic_dataset_business.json\", chunksize = 1000, lines = True)\n",
    "drop_cols = ['address', 'state', 'postal_code', 'latitude', 'longitude', 'stars', 'review_count', 'is_open','attributes', 'hours']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting non useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "chunks = []\n",
    "a = 0\n",
    "for chunk in df2:\n",
    "    a += 1\n",
    "    chunk_b = chunk.drop(drop_cols, axis = 1)\n",
    "    restas = chunk_b[chunk_b['categories'].str.contains('restaurant', case = False, na = False)]\n",
    "    chunks.append(restas)\n",
    "restaurants = pd.concat(chunks, ignore_index= True, join='outer')\n",
    "end = time.time()\n",
    "elapsed = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63961, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the reviews dataset\n",
    "* Remember we made the merge to use ONLY restaurants data, because there were data from other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_raw = pd.read_json(\"yelp_academic_dataset_review.json\", chunksize=100000, lines = True)\n",
    "drop_cols = ['review_id', 'user_id','useful', 'funny', 'cool', 'date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using merge instead of join because we want to join in another column other than the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for chunk in reviews_raw:\n",
    "    a += 1\n",
    "    reviews = chunk.drop(drop_cols, axis = 1)\n",
    "    data = restaurants.merge(reviews, left_on = 'business_id', right_on = 'business_id',how = 'inner')\n",
    "    if a == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally the data to be preprocessed (the \"text\" column, to be exact) \n",
    "TBD:\n",
    "* Delete all number 3 i.e neutral \n",
    "* Same number of positive as negatives\n",
    "* Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>categories</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pQeaRpvuhoEqudo3uymHIQ</td>\n",
       "      <td>The Empanadas House</td>\n",
       "      <td>Champaign</td>\n",
       "      <td>Ethnic Food, Food Trucks, Specialty Food, Impo...</td>\n",
       "      <td>2</td>\n",
       "      <td>I went to the place on Green St today. I went ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CsLQLiRoafpJPJSkNX2h5Q</td>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>Food, Restaurants, Grocery, Middle Eastern</td>\n",
       "      <td>3</td>\n",
       "      <td>Pretty decent middle eastern fare. The shelves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vjTVxnsQEZ34XjYNS-XUpA</td>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Food, Pretzels, Bakeries, Fast Food, Restaurants</td>\n",
       "      <td>5</td>\n",
       "      <td>I love Wetzel's Pretzels. And I can't imagine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fnZrZlqW1Z8iWgTVDfv_MA</td>\n",
       "      <td>Carl's Jr</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Mexican, Restaurants, Fast Food</td>\n",
       "      <td>1</td>\n",
       "      <td>I went here because of long line at IN n OUT a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fnZrZlqW1Z8iWgTVDfv_MA</td>\n",
       "      <td>Carl's Jr</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Mexican, Restaurants, Fast Food</td>\n",
       "      <td>2</td>\n",
       "      <td>Food was alright, but with only 1 person ahead...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                 name       city  \\\n",
       "0  pQeaRpvuhoEqudo3uymHIQ  The Empanadas House  Champaign   \n",
       "1  CsLQLiRoafpJPJSkNX2h5Q     Middle East Deli  Charlotte   \n",
       "2  vjTVxnsQEZ34XjYNS-XUpA    Wetzel's Pretzels    Phoenix   \n",
       "3  fnZrZlqW1Z8iWgTVDfv_MA            Carl's Jr  Las Vegas   \n",
       "4  fnZrZlqW1Z8iWgTVDfv_MA            Carl's Jr  Las Vegas   \n",
       "\n",
       "                                          categories  stars  \\\n",
       "0  Ethnic Food, Food Trucks, Specialty Food, Impo...      2   \n",
       "1         Food, Restaurants, Grocery, Middle Eastern      3   \n",
       "2   Food, Pretzels, Bakeries, Fast Food, Restaurants      5   \n",
       "3                    Mexican, Restaurants, Fast Food      1   \n",
       "4                    Mexican, Restaurants, Fast Food      2   \n",
       "\n",
       "                                                text  \n",
       "0  I went to the place on Green St today. I went ...  \n",
       "1  Pretty decent middle eastern fare. The shelves...  \n",
       "2  I love Wetzel's Pretzels. And I can't imagine ...  \n",
       "3  I went here because of long line at IN n OUT a...  \n",
       "4  Food was alright, but with only 1 person ahead...  "
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a toyset to work in trials from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    53\n",
       "4    47\n",
       "1    44\n",
       "3    38\n",
       "2    18\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.loc[:, ['name', 'stars', 'text']]\n",
    "data2['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some graphs to know the number of reviews by ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Delete the number 3's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data.drop(data[data['stars'] == 3].index, inplace = True)\n",
    "#data = data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We want just the reviews with 4-5 to be positive and the 1-2 to be negative, we do that on the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['Sentiment'] = data['stars'].apply(lambda x: 1 if x > 3 else 0)\n",
    "#data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Middle East Deli</td>\n",
       "      <td>4</td>\n",
       "      <td>Yummy authentic Lebanese food.  Havent tried m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Philthy Phillys</td>\n",
       "      <td>5</td>\n",
       "      <td>The cheese steak is great here. The guys cooki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>5</td>\n",
       "      <td>I have been had takeout sushi here quite often...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>1</td>\n",
       "      <td>Horrible.  I couldn't get my app scanned becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>5</td>\n",
       "      <td>I LOVE WETZEL'S PRETZELS.  My husband and I wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  stars                                               text\n",
       "0   Middle East Deli      4  Yummy authentic Lebanese food.  Havent tried m...\n",
       "1    Philthy Phillys      5  The cheese steak is great here. The guys cooki...\n",
       "2       Banzai Sushi      5  I have been had takeout sushi here quite often...\n",
       "3  Wetzel's Pretzels      1  Horrible.  I couldn't get my app scanned becau...\n",
       "4  Wetzel's Pretzels      5  I LOVE WETZEL'S PRETZELS.  My husband and I wi..."
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use \"3\" as negative to augment the data and  balance the classes? (optional)\n",
    "One could argue that the reviews that have not unconditionally positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_sample(data, no_samples):\n",
    "    neg_half = list((data[data['stars'] <=3]).index)\n",
    "    neg_half = neg_half[:no_samples]\n",
    "    pos_half = list(set(range(data.shape[0])) - set(neg_half))\n",
    "    pos_half = pos_half[0:len(neg_half)]\n",
    "    pos_half.extend(neg_half)\n",
    "    dropper = list(set(range(data.shape[0])) - set(pos_half))\n",
    "    data.drop(index = dropper, inplace = True)\n",
    "    data['Sentiment'] = data['stars'].apply(lambda x: 1 if x > 3 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here start the NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to make a bag of words, it can be done manually, but also with sklearn.\n",
    "\n",
    "**Steps:** I am trying to extract the nest tokens with the tokenizer from Potts and then feed that already \"clean tokens\" to the vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoC for NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0d40c6222516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mraw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data2' is not defined"
     ]
    }
   ],
   "source": [
    "raw = data2['text'][1:3]\n",
    "raw\n",
    "raw2 = data2['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "counter = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1fe0b28a6811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw2' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = []\n",
    "a = 0\n",
    "for i in raw2:\n",
    "    words = i.lower()\n",
    "    words = words.split()\n",
    "    for word in words:\n",
    "        a += 1\n",
    "        counter[word] += 1\n",
    "        if word not in tokenizer:\n",
    "            tokenizer.append(word)\n",
    "len(tokenizer)\n",
    "tokenizer\n",
    "words = raw2.split()\n",
    "for word in words:\n",
    "    a += 1\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the negation tagging, put the negation until ^[.:;!?]$ (until the punctuation mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ddcfffcf2aa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokens_nltk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw2' is not defined"
     ]
    }
   ],
   "source": [
    "#out of the box tokenizer and counter\n",
    "from nltk.probability import FreqDist\n",
    "counter = FreqDist()\n",
    "tokens_nltk = word_tokenize(raw2)\n",
    "\n",
    "tokenizer = []\n",
    "for word in tokens_nltk:\n",
    "    counter[word.lower()] += 1\n",
    "\n",
    "len(tokens_nltk)\n",
    "len(counter)\n",
    "counter['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter.B() #is the number of unique words (?)\n",
    "#x = counter.N #is the number of words\n",
    "#counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials for regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD:\n",
    "### Check the paper that is mentioned in notion by UCLondon\n",
    "\n",
    "* Delete all the reviews that are not in english\n",
    "* Same number of negative that as positive reviews for the sets\n",
    "* Tokenization and BoW creation (**BoW with frequency or with presence?**)\n",
    "* unigrams and bigrams\n",
    "* lower the case\n",
    "* POS tagging?\n",
    "* EDA como en potts con las palabras mas frecuentes en positives and negatives reviews\n",
    "* Handling negation? _NOT or with sentiment negative scoring?\n",
    "* See the book by Bing Lui for how to identify fake news, resonates with anomaly detection and identifying if a review is fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123456789', '987654321', '987654321']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'raw2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-53d2d7a1fa09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mraw2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'raw2' is not defined"
     ]
    }
   ],
   "source": [
    "string = \"\"\"Hello my Number is 123456789 and  \n",
    "             my friend's number is 987654321, and my number is also 987654321\"\"\"\n",
    "    \n",
    "# A sample regular expression to find digits.  \n",
    "regex = '\\d+'             \n",
    "    \n",
    "match = re.findall(regex, string)  \n",
    "print(match)\n",
    "raw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation tagging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesimist(text):\n",
    "    x = text\n",
    "    x = x.split()\n",
    "    hasta = 0\n",
    "    desde = 0\n",
    "    c = 0\n",
    "    j = 0\n",
    "    passer = False\n",
    "    for k in range(len(x)):\n",
    "        #print(j)\n",
    "        #print(k)\n",
    "        #if not passer:\n",
    "            #continue\n",
    "        passer = True\n",
    "\n",
    "        i = x[k]\n",
    "        negation_string = r\"\"\"\n",
    "        ^(?:never|no|nothing|nowhere|noone|none|not|\n",
    "            havent|hasnt|hadnt|cant|couldnt|shouldnt|\n",
    "            wont|wouldnt|but|doesnt|didnt|isnt|arent|aint\n",
    "        )$|n't\n",
    "        \"\"\"\n",
    "        #print(\"first\")\n",
    "        c +=1\n",
    "        #match = re.search(r'\\bthe\\b',i)\n",
    "        neg = re.compile(negation_string, re.VERBOSE | re.I | re.UNICODE)\n",
    "        match = neg.findall(i)\n",
    "        #print(i + \"i\")\n",
    "        #print('desde', desde)\n",
    "        #print(f'hasta menos desde es: {hasta - desde} y c es {c}')\n",
    "        if c < (hasta - desde):\n",
    "            continue\n",
    "        if match:\n",
    "            c = 0\n",
    "            desde = k + 1\n",
    "            #print(c - 1)\n",
    "            #j = i\n",
    "            jump = k\n",
    "            for j in range(jump, 100):\n",
    "\n",
    "                #print(j)\n",
    "                try:\n",
    "                    comma = re.search(r'[.:;!?]', x[j])\n",
    "                    #print(\"second\")\n",
    "                    if comma:\n",
    "                        hasta = j + 1\n",
    "                        c += 1\n",
    "                        \n",
    "                        #this try to avoid error if there is no punctuation error before the phrase ends\n",
    "                        try:\n",
    "                            for i in range(desde, hasta):\n",
    "                                repl = re.match(r'\\w+', x[i])\n",
    "                                x[i] = repl.group() + \"_NOT\"\n",
    "                            \n",
    "                            c = 0\n",
    "                            break\n",
    "                        except:\n",
    "                            #print(' '.join(x))\n",
    "                            c = 0\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "        if match:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "                #continue\n",
    "\n",
    "\n",
    "            #else:\n",
    "                #continue\n",
    "            #break\n",
    "    xx = ' '.join(x)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Already done*\n",
    "    1. Identify all the negation words on the regular expresion, can be taken from one paper.\n",
    "    2. Implement it i conjuction with the tokenizer and the stop words removal\n",
    "    3. run it for all the dataset\n",
    "    4. Balance the positive and negative classes on the data set that we are going to take to make all the trials.\n",
    "    5. Finish the identification of features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current status\n",
    "\n",
    "order of the pipeline (in **bold** what is already done)\n",
    "\n",
    "(balance the sample 50/50 in reviews)\n",
    "1. **Negator**\n",
    "2. **Tokenizer**\n",
    "3. **Stop words removal (kind of done, have to figure it out)**\n",
    "3. **BoW**\n",
    "\n",
    "*Follows: Select the features and extend the functionality for all the reviews*\n",
    "1. bigram\n",
    "2. Positive Tokens\n",
    "3. Negative Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trial = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function of negation taggin to each row\n",
    "trial[\"sample\"] = trial.loc[:, \"text\"].apply(pesimist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>categories</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lu7vtrp_bE9PnxWfA8g4Pg</td>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>Thornhill</td>\n",
       "      <td>Japanese, Fast Food, Food Court, Restaurants</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Sushi, and unbeatable prices! Only downf...</td>\n",
       "      <td>1</td>\n",
       "      <td>Great Sushi, and unbeatable prices! Only downf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lu7vtrp_bE9PnxWfA8g4Pg</td>\n",
       "      <td>Banzai Sushi</td>\n",
       "      <td>Thornhill</td>\n",
       "      <td>Japanese, Fast Food, Food Court, Restaurants</td>\n",
       "      <td>3</td>\n",
       "      <td>I don't listen to my father often when it come...</td>\n",
       "      <td>0</td>\n",
       "      <td>I don't listen_NOT to_NOT my_NOT father_NOT of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vjTVxnsQEZ34XjYNS-XUpA</td>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Food, Pretzels, Bakeries, Fast Food, Restaurants</td>\n",
       "      <td>4</td>\n",
       "      <td>Never heard of the cheese meltdown pretzel, bu...</td>\n",
       "      <td>1</td>\n",
       "      <td>Never heard_NOT of_NOT the_NOT cheese_NOT melt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vjTVxnsQEZ34XjYNS-XUpA</td>\n",
       "      <td>Wetzel's Pretzels</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Food, Pretzels, Bakeries, Fast Food, Restaurants</td>\n",
       "      <td>4</td>\n",
       "      <td>PV Mall's food court needs updating, but that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>PV Mall's food court needs updating, but that_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fnZrZlqW1Z8iWgTVDfv_MA</td>\n",
       "      <td>Carl's Jr</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Mexican, Restaurants, Fast Food</td>\n",
       "      <td>3</td>\n",
       "      <td>I haven't tried much on their menu but their c...</td>\n",
       "      <td>0</td>\n",
       "      <td>I haven't tried_NOT much_NOT on_NOT their_NOT ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id               name       city  \\\n",
       "0  lu7vtrp_bE9PnxWfA8g4Pg       Banzai Sushi  Thornhill   \n",
       "1  lu7vtrp_bE9PnxWfA8g4Pg       Banzai Sushi  Thornhill   \n",
       "2  vjTVxnsQEZ34XjYNS-XUpA  Wetzel's Pretzels    Phoenix   \n",
       "3  vjTVxnsQEZ34XjYNS-XUpA  Wetzel's Pretzels    Phoenix   \n",
       "4  fnZrZlqW1Z8iWgTVDfv_MA          Carl's Jr  Las Vegas   \n",
       "\n",
       "                                         categories  stars  \\\n",
       "0      Japanese, Fast Food, Food Court, Restaurants      5   \n",
       "1      Japanese, Fast Food, Food Court, Restaurants      3   \n",
       "2  Food, Pretzels, Bakeries, Fast Food, Restaurants      4   \n",
       "3  Food, Pretzels, Bakeries, Fast Food, Restaurants      4   \n",
       "4                   Mexican, Restaurants, Fast Food      3   \n",
       "\n",
       "                                                text  Sentiment  \\\n",
       "0  Great Sushi, and unbeatable prices! Only downf...          1   \n",
       "1  I don't listen to my father often when it come...          0   \n",
       "2  Never heard of the cheese meltdown pretzel, bu...          1   \n",
       "3  PV Mall's food court needs updating, but that ...          1   \n",
       "4  I haven't tried much on their menu but their c...          0   \n",
       "\n",
       "                                              sample  \n",
       "0  Great Sushi, and unbeatable prices! Only downf...  \n",
       "1  I don't listen_NOT to_NOT my_NOT father_NOT of...  \n",
       "2  Never heard_NOT of_NOT the_NOT cheese_NOT melt...  \n",
       "3  PV Mall's food court needs updating, but that_...  \n",
       "4  I haven't tried_NOT much_NOT on_NOT their_NOT ...  "
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.happyfuntokenizing import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiktokens(raw_data):\n",
    "    \"\"\"\n",
    "    Tokenizer function\n",
    "    \"\"\"\n",
    "    raw_data[\"sample\"] = raw_data.loc[:, \"text\"].apply(pesimist)\n",
    "    tok = Tokenizer()\n",
    "    raw_data[\"tokens\"] = raw_data.loc[:, \"sample\"].apply(tok.tokenize)\n",
    "    raw_data.reset_index(inplace = True)\n",
    "    print(raw_data[\"Sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preserve_case': False, 'all_in': False}"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tok.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing 'filler' words\n",
    "Counting the most common words over all the corpora we can tell that the most common ones, and thus, tne ones that may play a bid role on the classification, are not meaningful for us to discover which words are really expressing a positive or negative emotion, thus, the first 15 words are going to be removed from the corpus.\n",
    "\n",
    "In the following cells, the most common words are shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = trial['tokens'].to_numpy()\n",
    "review1 = reviews[0]\n",
    "rank = trial['Sentiment'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_words(raw_reviews, w_number):\n",
    "    \"\"\"\n",
    "    Creates a list with the most 'w_number' (number) of words on the whole corpus\n",
    "    ---------\n",
    "    raw_reviews : all the column of the raw reviews\n",
    "    w_number = number of most common words that wish to be extracted\n",
    "    \"\"\"\n",
    "    \n",
    "    commons = []\n",
    "    for word in counting:\n",
    "        commons.append(word[0])\n",
    "        if len(commons) == w_number:\n",
    "            break\n",
    "    return commons, counter\n",
    "#cc, count = most_common_words(reviews, 9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "Using the tokens, of course\n",
    "* Make the corpora\n",
    "* Make the vectors with word presence/frecuency. I think presenc may be better for vector-space representation\n",
    "\n",
    "TBD: check why is not working when the number of ommited words is omre than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restricted_corpus_builder(corpus_size, raw_reviews):\n",
    "    \"\"\"\n",
    "    Create the bag of words with the first 'corpus_size' most common words present on the reviews\n",
    "    -----------\n",
    "    corpus_size : number of words to be included on de bow\n",
    "    raw_reviews : all the column of the raw reviews\n",
    "    \"\"\"\n",
    "    bow = []\n",
    "    bow_counter = []\n",
    "    counter = FreqDist()\n",
    "    for review in raw_reviews:\n",
    "        for word in review:\n",
    "            counter[word] += 1\n",
    "    counting = counter.most_common(corpus_size)\n",
    "    for word in counting:\n",
    "        bow.append(word[0])\n",
    "        bow_counter.append(word[1])\n",
    "    return bow, bow_counter  \n",
    "bow, bow_counted = restricted_corpus_builder(3000, reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot showing the big frecuency gap between the most common words and the not so common\n",
    "The first 100 seem to be the most common among this domain (restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a43ff2310>]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot((np.linspace(0, len(bow_counted), num=len(bow_counted))), bow_counted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor word, numb in enumerate(zip(bow, bow_counted)):\\n    print(word, numb)\\n'"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for word, numb in enumerate(zip(bow, bow_counted)):\n",
    "    print(word, numb)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_builder(raw_reviews, most_common):\n",
    "    \"\"\"\n",
    "    Create the bag of words of all the words present on the reviews, ommiting the 'most_common' words as they are \n",
    "    conseidered as fillers with low influence on the classification\n",
    "    -----------\n",
    "    raw_reviews : all the column of the raw reviews\n",
    "    most_common : list of most common words that wish to be ommited\n",
    "    \"\"\"\n",
    "    main_corpus = []\n",
    "    for review in raw_reviews:\n",
    "        for word in review:\n",
    "            if word in most_common:\n",
    "                continue\n",
    "            if word not in main_corpus:\n",
    "                main_corpus.append(word)\n",
    "    return main_corpus\n",
    "#bow = corpus_builder(reviews, cc)\n",
    "#len(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(bow, ind_review, ommitted_words):\n",
    "    \"\"\"\n",
    "    Vectorize the review based on the counting of the words present on the review, \n",
    "    the appereances are counted on a dictionary, the value of the words (keys) that are not present remain as '0'\n",
    "    ----------\n",
    "    bow : bag of all the words in list (unique values)\n",
    "    ind_review = indiviual review to be vectorized\n",
    "    ommited_words: number of most common words that are going to be ommited\n",
    "    \"\"\"\n",
    "    ommitted = bow[:ommitted_words]\n",
    "    bow = bow[ommitted_words:]\n",
    "    counter = dict.fromkeys(bow, 0)\n",
    "    for word in ind_review:\n",
    "        try:\n",
    "            if word in ommitted_words:\n",
    "                continue\n",
    "            else:\n",
    "                counter[word] += 1\n",
    "        except:\n",
    "            pass\n",
    "    return list(counter.values()), ommitted\n",
    "#vv = vectorizer(bow, review1, cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_builder(bow, review_series, ommited_words):\n",
    "    \"\"\"\n",
    "    Creates the matrix of features based on the term frecuency vectors created by the function vectorizer\n",
    "    ----------\n",
    "    bow: bag of words\n",
    "    review_series : pandas object (series)  i.e dataframe[name_of_column]\n",
    "    ommited_words: number of most common words that are going to be ommited\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    i = 0\n",
    "    \n",
    "    for review in review_series:\n",
    "        word_vector, ommitted = vectorizer(bow, review, ommited_words)\n",
    "        if i < 1:\n",
    "            X = np.array([word_vector])\n",
    "            i += 1\n",
    "            continue\n",
    "        X = np.append(X, [word_vector], axis = 0)\n",
    "    print(ommitted)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for the creation of the feature matrix\n",
    "* features thus far: \n",
    "    * unigram i.e. word frecuency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counting' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-45b132d09c9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcommon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_common_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-242a07c00fca>\u001b[0m in \u001b[0;36mmost_common_words\u001b[0;34m(raw_reviews, w_number)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcommons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mcommons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommons\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mw_number\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'counting' is not defined"
     ]
    }
   ],
   "source": [
    "reviews = trial['tokens'].to_numpy()\n",
    "common = most_common_words(reviews, 9)\n",
    "bow = corpus_builder(reviews, common)\n",
    "raw_reviews = trial['tokens']\n",
    "X = matrix_builder(bow, raw_reviews, common)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Pipeline\n",
    "\n",
    "1. Data\n",
    "2. Tokens\n",
    "3. Bow\n",
    "3. Features (aka Matrix builder)\n",
    "4. Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    100\n",
      "0    100\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data2 = data.copy()\n",
    "balance_sample(data2, 100)\n",
    "tiktokens(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data2['tokens'].to_numpy()\n",
    "y = data2['Sentiment'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Construct the feature matrix (vectors)\n",
    "common = 0\n",
    "X = matrix_builder(bow, features, common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3000) ||| (200,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,\"|||\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here start the classifier\n",
    "\n",
    "* To start we are going to work only with unigrams, then we can add a lexicon to determine the sentiment score and use that as feature or also try with bigrams or somethign along those lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = SVC(C = 1e-5, kernel = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1e-05, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48484848484848486"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(X_test)\n",
    "accu = accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49393939393939396"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf2 = MultinomialNB()\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48484848484848486"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
